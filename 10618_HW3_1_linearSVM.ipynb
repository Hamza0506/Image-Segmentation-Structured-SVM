{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10618_HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arndmghsh/ImageSegmentation-StructuredSVM/blob/master/10618_HW3_1_linearSVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDKtlPYoRAqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install --upgrade ortools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkXXNvkBekZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnNyEsQ6hJzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip -uq \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledImages.zip\" -d \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledImages\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQkZPu-1j3cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip -uq \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledLabels.zip\" -d \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zxfCW9PO7oXB",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from ortools.linear_solver import pywraplp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4yT4lrdDyKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_CLASSES = 2\n",
        "NUM_FEATURES = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irbTR7XvPTs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VOCDataset(Dataset):\n",
        "    \"\"\"Class to store VOC semantic segmentation dataset\"\"\"\n",
        "\n",
        "    def __init__(self, image_dir, label_dir, file_list):\n",
        "\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        reader = open(file_list, \"r\")\n",
        "        self.files = []\n",
        "        for file in reader:\n",
        "            self.files.append(file.strip())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.files[idx]\n",
        "        # 0 stands for background, 1 for foreground\n",
        "        labels = np.load(os.path.join(self.label_dir, fname+\".npy\"))\n",
        "        labels[labels > 0.0] = 1.0\n",
        "        image = Image.open(os.path.join(self.image_dir, fname+\".jpg\"), \"r\")\n",
        "        sample = (TF.to_tensor(image), torch.LongTensor(labels))\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mv1YMAzPWN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "    \"\"\"Class defining AlexNet layers used for the convolutional network\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=2, padding=4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9umKmhR1Padf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FCNHead(nn.Sequential):\n",
        "    \"\"\"Class defining FCN (fully convolutional network) layers\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, channels):\n",
        "        inter_channels = in_channels // 4\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(inter_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv2d(inter_channels, channels, 1)\n",
        "        ]\n",
        "\n",
        "        super(FCNHead, self).__init__(*layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRX-E4THPjBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleSegmentationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Class defining end-to-end semantic segmentation model.\n",
        "    It combines AlexNet and FCN layers with interpolation for deconvolution.\n",
        "    This model is pretrained using cross-entropy loss.\n",
        "    After pre-training, use the get_repr() function to construct 32x32x100 feature tensors for each image\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_feat, n_classes):\n",
        "        super(SimpleSegmentationModel, self).__init__()\n",
        "        self.n_feat = n_feat\n",
        "        self.backbone = AlexNet()\n",
        "        self.classifier = FCNHead(256, n_feat)\n",
        "        self.linear = nn.Linear(n_feat, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_shape = x.shape[-2:]\n",
        "        features = self.backbone(x)\n",
        "        x = self.classifier(features)\n",
        "        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous().view(-1, self.n_feat)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_repr(self, x):\n",
        "        input_shape = x.shape[-2:]\n",
        "        features = self.backbone(x)\n",
        "        x = self.classifier(features)\n",
        "        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxiDrt8aPvcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearSVM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_feat, n_classes):\n",
        "        super(LinearSVM, self).__init__()\n",
        "        self.n_feat = n_feat\n",
        "        self.n_classes = n_classes\n",
        "        # TODO: Define weights for linear SVM\n",
        "        # self.w = torch.autograd.Variable(torch.rand(n_feat), requires_grad=True)\n",
        "        # self.b = torch.autograd.Variable(torch.rand(1), requires_grad=True)\n",
        "        self.linear = nn.Linear(n_feat, n_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # TODO: Define forward function for linear SVM\n",
        "        # x: 1 x 32 x 32 x 100, w: 100 , b: 1\n",
        "        # y_hat = torch.dot(x,self.w)+self.b\n",
        "        x = x.contiguous().view(-1, self.n_feat)\n",
        "        # x: 1024 x 100\n",
        "        y_hat = self.linear(x)      # returns 1024 x 2\n",
        "        return y_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djSlxCHmP0n4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StructSVM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_feat, n_classes, w, h):\n",
        "        super(StructSVM, self).__init__()\n",
        "        self.n_feat = n_feat\n",
        "        self.n_classes = n_classes\n",
        "        self.w = w\n",
        "        self.h = h\n",
        "\n",
        "        # TODO: Define weights for structured SVM\n",
        "\n",
        "\n",
        "    def forward(self, image):\n",
        "        # TODO: Define forward function for structured SVM\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rljo5bqJP3NK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_cnn(model, train_batches, test_batches, num_epochs):\n",
        "    \"\"\"\n",
        "    This function runs a training loop for the FCN semantic segmentation model\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss(weight=torch.Tensor([1, 4]))\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for i, batch in enumerate(train_batches):\n",
        "            optimizer.zero_grad()\n",
        "            images, labels = batch\n",
        "            # Images: torch.Size([1, 3, 32, 32]), Labels = torch.Size([1, 32, 32])\n",
        "            output = model(images)\n",
        "            # output: 32*32 x 2 = torch.Size([1024, 2])\n",
        "            labels = labels.contiguous().view(-1, 1).squeeze()\n",
        "            # labels: torch.Size([1024])\n",
        "            loss = criterion(output, labels)  # inputs: torch.Size([1024, 2]) and torch.Size([1024])\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # print(\"Epoch: {}, itr: {}\".format(epoch, i))\n",
        "        print(\"Training loss after epoch {}: {}\".format(epoch, total_loss/len(train_batches)))\n",
        "        test_cnn(model, test_batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcUqpbKxPnH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_cnn(model, test_batches):\n",
        "    \"\"\"\n",
        "        This function evaluates the FCN semantic segmentation model on the test set\n",
        "    \"\"\"\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    class_gold = [0.0] * NUM_CLASSES\n",
        "    class_pred = [0.0] * NUM_CLASSES\n",
        "    class_correct = [0.0] * NUM_CLASSES\n",
        "    for i, batch in enumerate(test_batches):\n",
        "        images, labels = batch\n",
        "        output = model(images)\n",
        "        _, output = torch.max(output, axis=1)\n",
        "        visualize_grayscale_image(output.view(32, 32).detach().numpy(), i)\n",
        "        output = output.squeeze().detach().numpy()\n",
        "        labels = labels.contiguous().view(-1, 1).squeeze().numpy()\n",
        "        cur_class_pred = np.unique(output, return_counts=True)\n",
        "        for key, val in zip(cur_class_pred[0], cur_class_pred[1]):\n",
        "            class_pred[key] += val\n",
        "        cur_class_gold = np.unique(labels, return_counts=True)\n",
        "        for key, val in zip(cur_class_gold[0], cur_class_gold[1]):\n",
        "            class_gold[key] += val\n",
        "        cur_correct = (output == labels).tolist()\n",
        "        for j, val in enumerate(cur_correct):\n",
        "            if val:\n",
        "                class_correct[labels[j]] += 1\n",
        "        correct += np.sum(cur_correct)\n",
        "        total += len(labels)\n",
        "    class_iou = [x/(y+z-x) for x, y, z in zip(class_correct, class_gold, class_pred)]\n",
        "    mean_iou = sum(class_iou) / len(class_correct)\n",
        "    print(\"Mean IOU: {}\".format(mean_iou))\n",
        "    print(\"Pixel Accuracy: {}\\n\".format(correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud2IwN-kP97o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_linear_svm(cnn_model, svm_model, train_batches, test_batches, num_epochs):\n",
        "    # TODO: Write a training loop for the linear SVM\n",
        "    # Keep in mind that the CNN model is needed to compute features, but it should not be finetuned\n",
        "    criterion = nn.MultiMarginLoss(weight=torch.Tensor([1, 4]))  # Class weights to handle class imbalance\n",
        "    optimizer = optim.Adam(svm_model.parameters(), lr=0.0001)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for i, batch in enumerate(train_batches):\n",
        "            optimizer.zero_grad()\n",
        "            images, labels = batch\n",
        "            # Images: torch.Size([1, 3, 32, 32]), Labels = torch.Size([1, 32, 32])\n",
        "            fcn_output = cnn_model.get_repr(images)  # returns 1 x 32 x 32 x 100\n",
        "            output = svm_model(fcn_output)       # torch.Size([1024, 2])\n",
        "            labels = labels.contiguous().view(-1, 1).squeeze()   # labels: torch.Size([1024])\n",
        "            loss = criterion(output, labels)    # inputs = torch.Size([1024, 2]) and torch.Size([1024])\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # print(\"Epoch: {}, itr: {}\".format(epoch, i))\n",
        "        print(\"Training loss after epoch {}: {}\".format(epoch, total_loss/len(train_batches)))\n",
        "        test_linear_svm(cnn_model, svm_model, test_batches)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDHSYvVUQAt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_linear_svm(cnn_model, svm_model, test_batches):\n",
        "    # TODO: Write a testing function for the linear SVM\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    class_gold = [0.0] * NUM_CLASSES\n",
        "    class_pred = [0.0] * NUM_CLASSES\n",
        "    class_correct = [0.0] * NUM_CLASSES\n",
        "    for i, batch in enumerate(test_batches):\n",
        "        images, labels = batch\n",
        "        fcn_output = cnn_model.get_repr(images)  # returns 1 x 32 x 32 x 100\n",
        "        output = svm_model(fcn_output)       # torch.Size([1024, 2])\n",
        "        _, output = torch.max(output, axis=1)\n",
        "        visualize_grayscale_image(output.view(32, 32).detach().numpy(), i)\n",
        "        output = output.squeeze().detach().numpy()\n",
        "        labels = labels.contiguous().view(-1, 1).squeeze().numpy()\n",
        "\n",
        "        cur_class_pred = np.unique(output, return_counts=True)\n",
        "        for key, val in zip(cur_class_pred[0], cur_class_pred[1]):\n",
        "            class_pred[key] += val\n",
        "        cur_class_gold = np.unique(labels, return_counts=True)\n",
        "        for key, val in zip(cur_class_gold[0], cur_class_gold[1]):\n",
        "            class_gold[key] += val\n",
        "        cur_correct = (output == labels).tolist()\n",
        "        for j, val in enumerate(cur_correct):\n",
        "            if val:\n",
        "                class_correct[labels[j]] += 1\n",
        "        correct += np.sum(cur_correct)\n",
        "        total += len(labels)\n",
        "    class_iou = [x/(y+z-x) for x, y, z in zip(class_correct, class_gold, class_pred)]\n",
        "    mean_iou = sum(class_iou) / len(class_correct)\n",
        "    print(\"Mean IOU: {}\".format(mean_iou))\n",
        "    print(\"Pixel Accuracy: {}\\n\".format(correct / total))\n",
        "\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXlETq3tQDBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Write a function to compute the structured hinge loss\n",
        "# using the max-scoring output from the ILP and the gold output\n",
        "# def compute_struct_svm_loss():\n",
        "#     return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIHVXtM8QGMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_struct_svm(cnn_model, svm_model, train_batches, test_batches, num_epochs):\n",
        "    # TODO: Write a training loop for the structured SVM\n",
        "    # Keep in mind that the CNN model is needed to compute features, but it should not be finetuned\n",
        "    return\n",
        "\n",
        "\n",
        "def test_struct_svm(cnn_model, svm_model, test_batches):\n",
        "    # TODO: Write a testing function for the structured SVM\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsdqzj1UQN6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_grayscale_image(image, file=None):\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    # Uncomment this to visualize image\n",
        "    # plt.show()\n",
        "    # Uncomment this to save image\n",
        "    # plt.savefig(str(file)+\".png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EurepqnDQQbv",
        "colab_type": "code",
        "outputId": "da8daf12-71e9-4d98-cce2-52e90836dde1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    seed = 42\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Uncomment following lines after providing appropriate paths\n",
        "    path_to_image_folder = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledImages\"\n",
        "    path_to_label_folder = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledLabels\"\n",
        "    file_with_train_ids = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/train.txt\"\n",
        "    file_with_test_ids = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/test.txt\"\n",
        "    \n",
        "    train_dataset = VOCDataset(path_to_image_folder, path_to_label_folder, file_with_train_ids)\n",
        "    test_dataset = VOCDataset(path_to_image_folder, path_to_label_folder, file_with_test_ids)\n",
        "\n",
        "    train_batches = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "    test_batches = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    cnn = SimpleSegmentationModel(NUM_FEATURES, NUM_CLASSES)\n",
        "    train_cnn(cnn, train_batches, test_batches, 2)\n",
        "    test_cnn(cnn, test_batches)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss after epoch 0: 0.6328337939384158\n",
            "Mean IOU: 0.44546086388144446\n",
            "Pixel Accuracy: 0.6883643331903945\n",
            "Training loss after epoch 1: 0.6287840272991443\n",
            "Mean IOU: 0.4303271701165907\n",
            "Pixel Accuracy: 0.6612115405231561\n",
            "Mean IOU: 0.4305581365280091\n",
            "Pixel Accuracy: 0.6614745256217839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnsRxJ2DoN69",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "44aa103d-2873-4e59-8aff-e9ae3b461b72"
      },
      "source": [
        "# TODO: Instantiate a linear SVM and call train/ test functions\n",
        "linear_svm = LinearSVM(n_feat = NUM_FEATURES, n_classes = NUM_CLASSES)\n",
        "train_linear_svm(cnn, linear_svm, train_batches, test_batches, num_epochs = 3)\n",
        "# TODO: Instantiate a structured SVM and call train/ test functions\n"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss after epoch 0: 0.6543165611350997\n",
            "Mean IOU: 0.39126035364619016\n",
            "Pixel Accuracy: 0.5990164022298456\n",
            "\n",
            "Training loss after epoch 1: 0.6287558046149594\n",
            "Mean IOU: 0.3921140230597239\n",
            "Pixel Accuracy: 0.6004686829974271\n",
            "\n",
            "Training loss after epoch 2: 0.6275776842760937\n",
            "Mean IOU: 0.3923062454558983\n",
            "Pixel Accuracy: 0.6009159251715266\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndqkJwy418f94p0JFkTwQnAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}