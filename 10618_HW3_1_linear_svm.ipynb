{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10618_HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arndmghsh/ImageSegmentation-StructuredSVM/blob/master/10618_HW3_1_linear_svm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDKtlPYoRAqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install --upgrade ortools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkXXNvkBekZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnNyEsQ6hJzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip -uq \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledImages.zip\" -d \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledImages\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQkZPu-1j3cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip -uq \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledLabels.zip\" -d \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zxfCW9PO7oXB",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from ortools.linear_solver import pywraplp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4yT4lrdDyKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_CLASSES = 2\n",
        "NUM_FEATURES = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irbTR7XvPTs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VOCDataset(Dataset):\n",
        "    \"\"\"Class to store VOC semantic segmentation dataset\"\"\"\n",
        "\n",
        "    def __init__(self, image_dir, label_dir, file_list):\n",
        "\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        reader = open(file_list, \"r\")\n",
        "        self.files = []\n",
        "        for file in reader:\n",
        "            self.files.append(file.strip())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.files[idx]\n",
        "        # 0 stands for background, 1 for foreground\n",
        "        labels = np.load(os.path.join(self.label_dir, fname+\".npy\"))\n",
        "        labels[labels > 0.0] = 1.0\n",
        "        image = Image.open(os.path.join(self.image_dir, fname+\".jpg\"), \"r\")\n",
        "        sample = (TF.to_tensor(image), torch.LongTensor(labels))\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mv1YMAzPWN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "    \"\"\"Class defining AlexNet layers using for the convolutional network\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=2, padding=4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9umKmhR1Padf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FCNHead(nn.Sequential):\n",
        "    \"\"\"Class defining FCN (fully convolutional network) layers\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, channels):\n",
        "        inter_channels = in_channels // 4\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(inter_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv2d(inter_channels, channels, 1)\n",
        "        ]\n",
        "\n",
        "        super(FCNHead, self).__init__(*layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRX-E4THPjBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleSegmentationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Class defining end-to-end semantic segmentation model.\n",
        "    It combines AlexNet and FCN layers with interpolation for deconvolution.\n",
        "    This model is pretrained using cross-entropy loss.\n",
        "    After pre-training, use the get_repr() function to construct 32x32x100 feature tensors for each image\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_feat, n_classes):\n",
        "        super(SimpleSegmentationModel, self).__init__()\n",
        "        self.n_feat = n_feat\n",
        "        self.backbone = AlexNet()\n",
        "        self.classifier = FCNHead(256, n_feat)\n",
        "        self.linear = nn.Linear(n_feat, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_shape = x.shape[-2:]\n",
        "        features = self.backbone(x)\n",
        "        x = self.classifier(features)\n",
        "        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous().view(-1, self.n_feat)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_repr(self, x):\n",
        "        input_shape = x.shape[-2:]\n",
        "        features = self.backbone(x)\n",
        "        x = self.classifier(features)\n",
        "        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxiDrt8aPvcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearSVM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_feat, n_classes):\n",
        "        super(LinearSVM, self).__init__()\n",
        "        self.n_feat = n_feat\n",
        "        self.n_classes = n_classes\n",
        "        # TODO: Define weights for linear SVM\n",
        "        # self.w = torch.autograd.Variable(torch.rand(n_feat), requires_grad=True)\n",
        "        # self.b = torch.autograd.Variable(torch.rand(1), requires_grad=True)\n",
        "        self.linear = nn.Linear(n_feat, n_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # TODO: Define forward function for linear SVM\n",
        "        # x: 1 x 32 x 32 x 100, w: 100 , b: 1\n",
        "        # y_hat = torch.dot(x,self.w)+self.b\n",
        "        x = x.contiguous().view(-1, self.n_feat)\n",
        "        # x: 1024 x 100\n",
        "        y_hat = self.linear(x)      # returns 1024 x 2\n",
        "        return y_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djSlxCHmP0n4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StructSVM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_feat, n_classes, w, h):\n",
        "        super(StructSVM, self).__init__()\n",
        "        self.n_feat = n_feat\n",
        "        self.n_classes = n_classes\n",
        "        self.w = w\n",
        "        self.h = h\n",
        "\n",
        "        # TODO: Define weights for structured SVM\n",
        "\n",
        "\n",
        "    def forward(self, image):\n",
        "        # TODO: Define forward function for structured SVM\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rljo5bqJP3NK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_cnn(model, train_batches, test_batches, num_epochs):\n",
        "    \"\"\"\n",
        "    This function runs a training loop for the FCN semantic segmentation model\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss(weight=torch.Tensor([1, 4]))\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for i, batch in enumerate(train_batches):\n",
        "            optimizer.zero_grad()\n",
        "            images, labels = batch\n",
        "            # Images: torch.Size([1, 3, 32, 32]), Labels = torch.Size([1, 32, 32])\n",
        "            output = model(images)\n",
        "            # output: 32*32 x 2 = torch.Size([1024, 2])\n",
        "            labels = labels.contiguous().view(-1, 1).squeeze()\n",
        "            # labels: torch.Size([1024])\n",
        "            loss = criterion(output, labels)  # inputs are: torch.Size([1024, 2]) and torch.Size([1024])\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # print(\"Epoch: {}, itr: {}\".format(epoch, i))\n",
        "        print(\"Training loss after epoch {}: {}\".format(epoch, total_loss/len(train_batches)))\n",
        "        test_cnn(model, test_batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcUqpbKxPnH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_cnn(model, test_batches):\n",
        "    \"\"\"\n",
        "        This function evaluates the FCN semantic segmentation model on the test set\n",
        "    \"\"\"\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    class_gold = [0.0] * NUM_CLASSES\n",
        "    class_pred = [0.0] * NUM_CLASSES\n",
        "    class_correct = [0.0] * NUM_CLASSES\n",
        "    for i, batch in enumerate(test_batches):\n",
        "        images, labels = batch\n",
        "        output = model(images)\n",
        "        _, output = torch.max(output, axis=1)\n",
        "        visualize_grayscale_image(output.view(32, 32).detach().numpy(), i)\n",
        "        # plt.imshow(images[0,:,:,:].permute(1,2,0))\n",
        "        output = output.squeeze().detach().numpy()\n",
        "        labels = labels.contiguous().view(-1, 1).squeeze().numpy()\n",
        "        class_pred = [x + y for x, y in zip(class_pred, np.unique(output, return_counts=True)[1])]\n",
        "        class_gold = [x + y for x, y in zip(class_gold, np.unique(labels, return_counts=True)[1])]\n",
        "        cur_correct = (output == labels).tolist()\n",
        "        for j, val in enumerate(cur_correct):\n",
        "            if val:\n",
        "                class_correct[labels[j]] += 1\n",
        "        correct += np.sum(cur_correct)\n",
        "        total += len(labels)\n",
        "    class_iou = [x/(y+z-x) for x, y, z in zip(class_correct, class_gold, class_pred)]\n",
        "    mean_iou = sum(class_iou) / len(class_correct)\n",
        "    print(\"Mean IOU: {}\".format(mean_iou))\n",
        "    print(\"Pixel Accuracy: {}\".format(correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud2IwN-kP97o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_linear_svm(cnn_model, svm_model, train_batches, test_batches, num_epochs):\n",
        "    # TODO: Write a training loop for the linear SVM\n",
        "    # Keep in mind that the CNN model is needed to compute features, but it should not be finetuned\n",
        "    criterion = nn.MultiMarginLoss(weight=torch.Tensor([1, 4]))  # Class weights to handle class imbalance\n",
        "    optimizer = optim.Adam(svm_model.parameters(), lr=0.0001)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for i, batch in enumerate(train_batches):\n",
        "            optimizer.zero_grad()\n",
        "            images, labels = batch\n",
        "            # Images: torch.Size([1, 3, 32, 32]), Labels = torch.Size([1, 32, 32])\n",
        "            fcn_output = cnn_model.get_repr(images)  # returns 1 x 32 x 32 x 100\n",
        "            output = svm_model(fcn_output)       # torch.Size([1024, 2])\n",
        "            labels = labels.contiguous().view(-1, 1).squeeze()   # labels: torch.Size([1024])\n",
        "            loss = criterion(output, labels)    # inputs = torch.Size([1024, 2]) and torch.Size([1024])\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # print(\"Epoch: {}, itr: {}\".format(epoch, i))\n",
        "        print(\"Training loss after epoch {}: {}\".format(epoch, total_loss/len(train_batches)))\n",
        "        test_linear_svm(cnn_model, svm_model, test_batches)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDHSYvVUQAt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_linear_svm(cnn_model, svm_model, test_batches):\n",
        "    # TODO: Write a testing function for the linear SVM\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    class_gold = [0.0] * NUM_CLASSES\n",
        "    class_pred = [0.0] * NUM_CLASSES\n",
        "    class_correct = [0.0] * NUM_CLASSES\n",
        "    for i, batch in enumerate(test_batches):\n",
        "        images, labels = batch\n",
        "        # output = model(images)   # output: 32*32 x 2 = torch.Size([1024, 2])\n",
        "        fcn_output = cnn_model.get_repr(images)  # returns 1 x 32 x 32 x 100\n",
        "        output = svm_model(fcn_output)       # torch.Size([1024, 2])\n",
        "        _, output = torch.max(output, axis=1)\n",
        "        visualize_grayscale_image(output.view(32, 32).detach().numpy(), i)\n",
        "        output = output.squeeze().detach().numpy()\n",
        "        labels = labels.contiguous().view(-1, 1).squeeze().numpy()\n",
        "        class_pred = [x + y for x, y in zip(class_pred, np.unique(output, return_counts=True)[1])]\n",
        "        class_gold = [x + y for x, y in zip(class_gold, np.unique(labels, return_counts=True)[1])]\n",
        "        cur_correct = (output == labels).tolist()\n",
        "        for j, val in enumerate(cur_correct):\n",
        "            if val:\n",
        "                class_correct[labels[j]] += 1\n",
        "        correct += np.sum(cur_correct)\n",
        "        total += len(labels)\n",
        "    class_iou = [x/(y+z-x) for x, y, z in zip(class_correct, class_gold, class_pred)]\n",
        "    mean_iou = sum(class_iou) / len(class_correct)\n",
        "    print(\"Mean IOU: {}\".format(mean_iou))\n",
        "    print(\"Pixel Accuracy: {}\".format(correct / total))\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXlETq3tQDBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Write a function to compute the structured hinge loss\n",
        "# using the max-scoring output from the ILP and the gold output\n",
        "# def compute_struct_svm_loss():\n",
        "#     return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIHVXtM8QGMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_struct_svm(cnn_model, svm_model, train_batches, test_batches, num_epochs):\n",
        "    # TODO: Write a training loop for the structured SVM\n",
        "    # Keep in mind that the CNN model is needed to compute features, but it should not be finetuned\n",
        "    return\n",
        "\n",
        "\n",
        "def test_struct_svm(cnn_model, svm_model, test_batches):\n",
        "    # TODO: Write a testing function for the structured SVM\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsdqzj1UQN6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_grayscale_image(image, file=None):\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    # Uncomment this to visualize image\n",
        "    # plt.show()\n",
        "    # Uncomment this to save image\n",
        "    # plt.savefig(str(file)+\".png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EurepqnDQQbv",
        "colab_type": "code",
        "outputId": "fd30559d-bf5b-405f-aa03-f19e681fa2bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    seed = 42\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Uncomment following lines after providing appropriate paths\n",
        "    path_to_image_folder = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledImages\"\n",
        "    path_to_label_folder = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledLabels\"\n",
        "    file_with_train_ids = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/train.txt\"\n",
        "    file_with_test_ids = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/test.txt\"\n",
        "    \n",
        "    train_dataset = VOCDataset(path_to_image_folder, path_to_label_folder, file_with_train_ids)\n",
        "    test_dataset = VOCDataset(path_to_image_folder, path_to_label_folder, file_with_test_ids)\n",
        "\n",
        "    train_batches = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "    test_batches = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    cnn = SimpleSegmentationModel(NUM_FEATURES, NUM_CLASSES)\n",
        "    train_cnn(cnn, train_batches, test_batches, 2)\n",
        "    test_cnn(cnn, test_batches)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss after epoch 0: 0.6328337939384158\n",
            "Mean IOU: 0.32761636849498355\n",
            "Pixel Accuracy: 0.6883643331903945\n",
            "Training loss after epoch 1: 0.6287840272991443\n",
            "Mean IOU: 0.31030672959325317\n",
            "Pixel Accuracy: 0.6612115405231561\n",
            "Mean IOU: 0.31044971262211685\n",
            "Pixel Accuracy: 0.6614745256217839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnsRxJ2DoN69",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "252f1ce4-9c55-47f1-b010-8d4ac617040b"
      },
      "source": [
        "# TODO: Instantiate a linear SVM and call train/ test functions\n",
        "linear_svm = LinearSVM(n_feat = NUM_FEATURES, n_classes = NUM_CLASSES)\n",
        "train_linear_svm(cnn, linear_svm, train_batches, test_batches, num_epochs = 3)\n",
        "# TODO: Instantiate a structured SVM and call train/ test functions\n"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss after epoch 0: 0.6570169235887445\n",
            "Mean IOU: 0.2749018789064071\n",
            "Pixel Accuracy: 0.6063113073542024\n",
            "Training loss after epoch 1: 0.629271684388746\n",
            "Mean IOU: 0.2743702647704671\n",
            "Pixel Accuracy: 0.6055123016723842\n",
            "Training loss after epoch 2: 0.6283314800633382\n",
            "Mean IOU: 0.2760249980859046\n",
            "Pixel Accuracy: 0.6079897218053173\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wN2/kJDozA1MAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx-_unxEF8fH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}