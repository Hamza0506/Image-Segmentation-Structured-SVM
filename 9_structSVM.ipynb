{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10618_HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arndmghsh/ImageSegmentation-StructuredSVM/blob/master/10618_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDKtlPYoRAqi",
        "colab_type": "code",
        "outputId": "594aa1f4-b629-4a5f-ef49-9de8a587af28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip install --upgrade ortools"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ortools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/c2/81a18b1ee4e9c8401fa2c9bc6fcd1b59b1c30ff7a1b214aff84d718d95c5/ortools-7.4.7247-cp36-cp36m-manylinux1_x86_64.whl (27.7MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7MB 85kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf>=3.10.0 in /usr/local/lib/python3.6/dist-packages (from ortools) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from ortools) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.10.0->ortools) (41.4.0)\n",
            "Installing collected packages: ortools\n",
            "Successfully installed ortools-7.4.7247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkXXNvkBekZT",
        "colab_type": "code",
        "outputId": "995cbebf-9c6a-4a48-b177-e46ec4e10584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnNyEsQ6hJzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip -uq \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledImages.zip\" -d \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledImages\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQkZPu-1j3cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip -uq \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledLabels.zip\" -d \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zxfCW9PO7oXB",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from ortools.linear_solver import pywraplp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4yT4lrdDyKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_CLASSES = 2\n",
        "NUM_FEATURES = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irbTR7XvPTs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VOCDataset(Dataset):\n",
        "    \"\"\"Class to store VOC semantic segmentation dataset\"\"\"\n",
        "\n",
        "    def __init__(self, image_dir, label_dir, file_list):\n",
        "\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        reader = open(file_list, \"r\")\n",
        "        self.files = []\n",
        "        for file in reader:\n",
        "            self.files.append(file.strip())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.files[idx]\n",
        "        # 0 stands for background, 1 for foreground\n",
        "        labels = np.load(os.path.join(self.label_dir, fname+\".npy\"))\n",
        "        labels[labels > 0.0] = 1.0\n",
        "        image = Image.open(os.path.join(self.image_dir, fname+\".jpg\"), \"r\")\n",
        "        sample = (TF.to_tensor(image), torch.LongTensor(labels))\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mv1YMAzPWN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "    \"\"\"Class defining AlexNet layers used for the convolutional network\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=2, padding=4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9umKmhR1Padf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FCNHead(nn.Sequential):\n",
        "    \"\"\"Class defining FCN (fully convolutional network) layers\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, channels):\n",
        "        inter_channels = in_channels // 4\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(inter_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv2d(inter_channels, channels, 1)\n",
        "        ]\n",
        "\n",
        "        super(FCNHead, self).__init__(*layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRX-E4THPjBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleSegmentationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Class defining end-to-end semantic segmentation model.\n",
        "    It combines AlexNet and FCN layers with interpolation for deconvolution.\n",
        "    This model is pretrained using cross-entropy loss.\n",
        "    After pre-training, use the get_repr() function to construct 32x32x100 feature tensors for each image\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_feat, n_classes):\n",
        "        super(SimpleSegmentationModel, self).__init__()\n",
        "        self.n_feat = n_feat\n",
        "        self.backbone = AlexNet()\n",
        "        self.classifier = FCNHead(256, n_feat)\n",
        "        self.linear = nn.Linear(n_feat, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_shape = x.shape[-2:]\n",
        "        features = self.backbone(x)\n",
        "        x = self.classifier(features)\n",
        "        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous().view(-1, self.n_feat)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_repr(self, x):\n",
        "        input_shape = x.shape[-2:]\n",
        "        features = self.backbone(x)\n",
        "        x = self.classifier(features)\n",
        "        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxiDrt8aPvcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearSVM(nn.Module):\n",
        "    def __init__(self, n_feat, n_classes):\n",
        "        super(LinearSVM, self).__init__()\n",
        "        self.n_feat = n_feat\n",
        "        self.n_classes = n_classes\n",
        "        # TODO: Define weights for linear SVM\n",
        "        # self.w = torch.autograd.Variable(torch.rand(n_feat), requires_grad=True)\n",
        "        # self.b = torch.autograd.Variable(torch.rand(1), requires_grad=True)\n",
        "        self.linear = nn.Linear(n_feat, n_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # TODO: Define forward function for linear SVM\n",
        "        # x: 1 x 32 x 32 x 100\n",
        "        x = x.contiguous().view(-1, self.n_feat)\n",
        "        # x: 1024 x 100\n",
        "        y_hat = self.linear(x)      # returns 1024 x 2\n",
        "        return y_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djSlxCHmP0n4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StructSVM(nn.Module):\n",
        "    def __init__(self, n_feat, n_classes, w, h):\n",
        "        super(StructSVM, self).__init__()\n",
        "        self.n_feat = n_feat\n",
        "        self.n_classes = n_classes\n",
        "        self.w = w\n",
        "        self.h = h\n",
        "        # TODO: Define weights for structured SVM\n",
        "        self.lin100 = nn.Linear(n_feat, n_classes)\n",
        "        self.lin200 = nn.Linear(2*n_feat, n_classes)\n",
        "\n",
        "    def forward(self, image):\n",
        "        # TODO: Define forward function for structured SVM\n",
        "        # image: 1 x 32 x 32 x 100\n",
        "        image = image.squeeze() # torch.Size([32, 32, 100])\n",
        "        unary = self.lin100(image)  # torch.Size([32, 32, 2])\n",
        "        # [i,j] and [i,j+1]\n",
        "        edge1 = torch.zeros(self.w, self.h-1, 2)\n",
        "        for i in range(self.w):\n",
        "          for j in range(self.h-1):\n",
        "            # image[i,j].shape, image[i,j+1].shape =  torch.Size([100])\n",
        "            concat = torch.cat((image[i,j], image[i,j+1]), dim=0)\n",
        "            # concat shape = torch.Size([200])\n",
        "            edge1[i,j] = self.lin200(concat)\n",
        "        # [i,j] and [i+1,j]\n",
        "        edge2 = torch.zeros(self.w-1, self.h, 2)\n",
        "        for i in range(self.w-1):\n",
        "          for j in range(self.h):\n",
        "            concat = torch.cat((image[i,j], image[i+1,j]), dim=0)\n",
        "            edge2[i,j] = self.lin200(concat)\n",
        "        # Unary, edge1, edge2 = torch.Size([32, 32, 2]) torch.Size([32, 31, 2]) torch.Size([31, 32, 2])\n",
        "        return unary, edge1, edge2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rljo5bqJP3NK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_cnn(model, train_batches, test_batches, num_epochs):\n",
        "    \"\"\"\n",
        "    This function runs a training loop for the FCN semantic segmentation model\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss(weight=torch.Tensor([1, 4]))\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for i, batch in enumerate(train_batches):\n",
        "            optimizer.zero_grad()\n",
        "            images, labels = batch\n",
        "            # Images: torch.Size([1, 3, 32, 32]), Labels = torch.Size([1, 32, 32])\n",
        "            output = model(images)\n",
        "            # output: 32*32 x 2 = torch.Size([1024, 2])\n",
        "            labels = labels.contiguous().view(-1, 1).squeeze()\n",
        "            # labels: torch.Size([1024])\n",
        "            loss = criterion(output, labels)  # inputs: torch.Size([1024, 2]) and torch.Size([1024])\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(\"Epoch: {}, itr: {}\".format(epoch, i))\n",
        "        print(\"Training loss after epoch {}: {}\".format(epoch, total_loss/len(train_batches)))\n",
        "        test_cnn(model, train_batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcUqpbKxPnH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_cnn(model, test_batches):\n",
        "    \"\"\"\n",
        "        This function evaluates the FCN semantic segmentation model on the test set\n",
        "    \"\"\"\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    class_gold = [0.0] * NUM_CLASSES\n",
        "    class_pred = [0.0] * NUM_CLASSES\n",
        "    class_correct = [0.0] * NUM_CLASSES\n",
        "    for i, batch in enumerate(test_batches):\n",
        "        images, labels = batch\n",
        "        output = model(images)\n",
        "        _, output = torch.max(output, axis=1)\n",
        "        visualize_grayscale_image(output.view(32, 32).detach().numpy(), i)\n",
        "        output = output.squeeze().detach().numpy()\n",
        "        labels = labels.contiguous().view(-1, 1).squeeze().numpy()\n",
        "        cur_class_pred = np.unique(output, return_counts=True)\n",
        "        for key, val in zip(cur_class_pred[0], cur_class_pred[1]):\n",
        "            class_pred[key] += val\n",
        "        cur_class_gold = np.unique(labels, return_counts=True)\n",
        "        for key, val in zip(cur_class_gold[0], cur_class_gold[1]):\n",
        "            class_gold[key] += val\n",
        "        cur_correct = (output == labels).tolist()\n",
        "        for j, val in enumerate(cur_correct):\n",
        "            if val:\n",
        "                class_correct[labels[j]] += 1\n",
        "        correct += np.sum(cur_correct)\n",
        "        total += len(labels)\n",
        "    class_iou = [x/(y+z-x) for x, y, z in zip(class_correct, class_gold, class_pred)]\n",
        "    mean_iou = sum(class_iou) / len(class_correct)\n",
        "    print(\"Mean IOU: {}\".format(mean_iou))\n",
        "    print(\"Pixel Accuracy: {}\\n\".format(correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud2IwN-kP97o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_linear_svm(cnn_model, svm_model, train_batches, test_batches, num_epochs):\n",
        "    # TODO: Write a training loop for the linear SVM\n",
        "    # Keep in mind that the CNN model is needed to compute features, but it should not be finetuned\n",
        "    criterion = nn.MultiMarginLoss(weight=torch.Tensor([1, 4]))  # Class weights to handle class imbalance\n",
        "    optimizer = optim.Adam(svm_model.parameters(), lr=0.0001)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for i, batch in enumerate(train_batches):\n",
        "            optimizer.zero_grad()\n",
        "            images, labels = batch\n",
        "            # Images: torch.Size([1, 3, 32, 32]), Labels = torch.Size([1, 32, 32])\n",
        "            fcn_output = cnn_model.get_repr(images)  # returns 1 x 32 x 32 x 100\n",
        "            output = svm_model(fcn_output)       # torch.Size([1024, 2])\n",
        "            labels = labels.contiguous().view(-1, 1).squeeze()   # labels: torch.Size([1024])\n",
        "            loss = criterion(output, labels)    # inputs = torch.Size([1024, 2]) and torch.Size([1024])\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # print(\"Epoch: {}, itr: {}\".format(epoch, i))\n",
        "        print(\"Training loss after epoch {}: {}\".format(epoch, total_loss/len(train_batches)))\n",
        "        test_linear_svm(cnn_model, svm_model, train_batches, criterion)\n",
        "        test_linear_svm(cnn_model, svm_model, test_batches, criterion)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDHSYvVUQAt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_linear_svm(cnn_model, svm_model, test_batches, criterion):\n",
        "    # TODO: Write a testing function for the linear SVM\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    class_gold = [0.0] * NUM_CLASSES\n",
        "    class_pred = [0.0] * NUM_CLASSES\n",
        "    class_correct = [0.0] * NUM_CLASSES\n",
        "    total_loss = 0.0\n",
        "    for i, batch in enumerate(test_batches):\n",
        "        images, labels = batch\n",
        "        fcn_output = cnn_model.get_repr(images)  # returns 1 x 32 x 32 x 100\n",
        "        output = svm_model(fcn_output)       # torch.Size([1024, 2])\n",
        "\n",
        "        labels = labels.contiguous().view(-1, 1).squeeze()   # labels: torch.Size([1024])\n",
        "        loss = criterion(output, labels)    # inputs = torch.Size([1024, 2]) and torch.Size([1024])\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        _, output = torch.max(output, axis=1)\n",
        "        visualize_grayscale_image(output.view(32, 32).detach().numpy(), i)\n",
        "        output = output.squeeze().detach().numpy()\n",
        "        labels = labels.contiguous().view(-1, 1).squeeze().numpy()\n",
        "\n",
        "        cur_class_pred = np.unique(output, return_counts=True)\n",
        "        for key, val in zip(cur_class_pred[0], cur_class_pred[1]):\n",
        "            class_pred[key] += val\n",
        "        cur_class_gold = np.unique(labels, return_counts=True)\n",
        "        for key, val in zip(cur_class_gold[0], cur_class_gold[1]):\n",
        "            class_gold[key] += val\n",
        "        cur_correct = (output == labels).tolist()\n",
        "        for j, val in enumerate(cur_correct):\n",
        "            if val:\n",
        "                class_correct[labels[j]] += 1\n",
        "        correct += np.sum(cur_correct)\n",
        "        total += len(labels)\n",
        "    class_iou = [x/(y+z-x) for x, y, z in zip(class_correct, class_gold, class_pred)]\n",
        "    mean_iou = sum(class_iou) / len(class_correct)\n",
        "    print(\"Mean IOU: {}\".format(mean_iou))\n",
        "    print(\"Pixel Accuracy: {}\".format(correct / total))\n",
        "    print(\"loss: {}\\n\".format(total_loss/len(test_batches)))\n",
        "\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-FR5TxbLTTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_aug_map_inference(solver, label_ind, unary, edge1, edge2, x_assign, y1_assign, y2_assign):\n",
        "    # Define objective\n",
        "    obj = 0.0\n",
        "    for i in range(32):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          obj += unary[i,j,c]*x_assign[i,j,c]\n",
        "          obj += (1/(32*32*2))*(label_ind[i,j,c]*(1-x_assign[i,j,c]) \n",
        "                                        + (1-label_ind[i,j,c])*x_assign[i,j,c])\n",
        "    for i in range(32):\n",
        "      for j in range(31):\n",
        "        for c in range(2):\n",
        "          obj += edge1[i,j,c]*y1_assign[i,j,c]\n",
        "    for i in range(31):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          obj += edge2[i,j,c]*y2_assign[i,j,c]\n",
        "    \n",
        "    solver.Maximize(obj)\n",
        "    \n",
        "    # Solve ILP again\n",
        "    result = solver.Solve()\n",
        "    assert result == pywraplp.Solver.OPTIMAL\n",
        "    assert solver.VerifySolution(1e-7, True)\n",
        "\n",
        "    # Gather the optimal assignment\n",
        "    final_x_assign = torch.zeros((32, 32, 2))\n",
        "    final_y1_assign = torch.zeros((32, 31, 2))\n",
        "    final_y2_assign = torch.zeros((31, 32, 2))\n",
        "    for i in range(32):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          final_x_assign[i, j, c] = x_assign[i,j,c].solution_value()\n",
        "    for i in range(32):\n",
        "      for j in range(31):\n",
        "        for c in range(2):\n",
        "          final_y1_assign[i, j, c] = y1_assign[i,j,c].solution_value()\n",
        "    for i in range(31):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          final_y2_assign[i, j, c] = y2_assign[i,j,c].solution_value()\n",
        "\n",
        "    return final_x_assign, final_y1_assign, final_y2_assign"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXlETq3tQDBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Write a function to compute the structured hinge loss\n",
        "# using the max-scoring output from the ILP and the gold output\n",
        "def compute_struct_svm_loss(unary, edge1, edge2, label_ind, edge1_ind, edge2_ind,\n",
        "                                           final_x_assign, final_y1_assign, final_y2_assign):\n",
        "  # Score term (Predicted)\n",
        "  score_pred = 0.0\n",
        "  score_pred += torch.sum(unary*final_x_assign)\n",
        "  score_pred += torch.sum(edge1*final_y1_assign)\n",
        "  score_pred += torch.sum(edge2*final_y2_assign)\n",
        "  # Hamming loss term\n",
        "  hloss = 0.0\n",
        "  for i in range(32):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          if(final_x_assign[i,j,c] != label_ind[i,j,c]):\n",
        "            hloss += 1\n",
        "  hloss = hloss/(32*32*2)\n",
        "  # Score term (Gold)\n",
        "  score_gold = 0.0\n",
        "  label_ind = torch.tensor(label_ind)\n",
        "  score_gold += torch.sum(unary*label_ind)\n",
        "  score_gold += torch.sum(edge1*edge1_ind)\n",
        "  score_gold += torch.sum(edge2*edge2_ind)\n",
        "  # Total loss\n",
        "  total_loss = score_pred + hloss - score_gold\n",
        "  return  F.relu(total_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIHVXtM8QGMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_struct_svm(cnn_model, svm_model, train_batches, test_batches, num_epochs):\n",
        "    # TODO: Write a training loop for the structured SVM\n",
        "    # Keep in mind that the CNN model is needed to compute features, but it should not be finetuned\n",
        "    # Define ILP Solver\n",
        "    solver = pywraplp.Solver('LinearExample', pywraplp.Solver.GLOP_LINEAR_PROGRAMMING)\n",
        "    # Define assignment variables\n",
        "    x_assign = {}\n",
        "    for i in range(32):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          x_assign[i, j, c] = solver.NumVar(0.0, 1.0, 'x[%d,%d, %d]' % (i, j, c))\n",
        "    # Define assignment variables\n",
        "    y1_assign = {}\n",
        "    for i in range(32):\n",
        "      for j in range(31):\n",
        "        for c in range(2):\n",
        "          y1_assign[i, j, c] = solver.NumVar(0.0, 1.0, 'y1[%d,%d, %d]' % (i, j, c))\n",
        "    # Define assignment variables\n",
        "    y2_assign = {}\n",
        "    for i in range(31):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          y2_assign[i, j, c] = solver.NumVar(0.0, 1.0, 'y2[%d,%d, %d]' % (i, j, c))\n",
        "    # Add constraints\n",
        "    for i in range(32):\n",
        "      for j in range(32):\n",
        "        solver.Add(solver.Sum([x_assign[i,j,c] for c in range(2)])==1)\n",
        "    # Add constraints\n",
        "    for i in range(32):\n",
        "      for j in range(31):\n",
        "        for c in range(2):\n",
        "          solver.Add(y1_assign[i,j,c]<=x_assign[i,j,c])\n",
        "          solver.Add(y1_assign[i,j,c]<=x_assign[i,j+1,c])\n",
        "    # Add constraints\n",
        "    for i in range(31):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          solver.Add(y2_assign[i,j,c]<=x_assign[i,j,c])\n",
        "          solver.Add(y2_assign[i,j,c]<=x_assign[i+1,j,c])\n",
        "\n",
        "    optimizer = optim.Adam(svm_model.parameters(), lr=0.0001)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for itr, batch in enumerate(train_batches):\n",
        "            optimizer.zero_grad()\n",
        "            images, labels = batch\n",
        "            # Images: torch.Size([1, 3, 32, 32]), Labels = torch.Size([1, 32, 32])\n",
        "            fcn_output = cnn_model.get_repr(images)  # returns 1 x 32 x 32 x 100\n",
        "            unary, edge1, edge2 = svm_model(fcn_output)\n",
        "            # Unary, edge1, edge2 = torch.Size([32, 32, 2]) torch.Size([32, 31, 2]) torch.Size([31, 32, 2])\n",
        "            labels = labels.squeeze()   # Labels = torch.Size([32, 32])\n",
        "\n",
        "            # Perform loss-augmented MAP inference via ILP\n",
        "            unary_np = unary.detach().numpy()\n",
        "            edge1_np = edge1.detach().numpy()\n",
        "            edge2_np = edge2.detach().numpy()\n",
        "            # Construct the Gold X indicators\n",
        "            label_ind = np.zeros((32,32,2))\n",
        "            for i in range(32):\n",
        "              for j in range(32):\n",
        "                label_ind[i,j,labels[i,j]] = 1\n",
        "            # Loss-augmented MAP inference\n",
        "            final_x_assign, final_y1_assign, final_y2_assign = loss_aug_map_inference(solver, label_ind, unary_np, \n",
        "                                                                        edge1_np, edge2_np, x_assign, y1_assign, y2_assign)\n",
        "            # Construct the Gold Y indicators\n",
        "            edge1_ind = torch.zeros((32,31,2))\n",
        "            edge2_ind = torch.zeros((31,32,2))\n",
        "            for i in range(32):\n",
        "              for j in range(31):\n",
        "                if labels[i,j]==1 and labels[i,j+1]==1:\n",
        "                  edge1_ind[i,j,1] = 1\n",
        "                if labels[i,j]==0 and labels[i,j+1]==0:\n",
        "                  edge1_ind[i,j,0] = 1\n",
        "            for i in range(31):\n",
        "              for j in range(32):\n",
        "                if labels[i,j]==1 and labels[i+1,j]==1:\n",
        "                  edge2_ind[i,j,1] = 1\n",
        "                if labels[i,j]==0 and labels[i+1,j]==0:\n",
        "                  edge2_ind[i,j,0] = 1\n",
        "            # Structured Hinge Loss\n",
        "            loss = compute_struct_svm_loss(unary, edge1, edge2, label_ind, edge1_ind, edge2_ind,\n",
        "                                           final_x_assign, final_y1_assign, final_y2_assign)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(\"Epoch: {}, itr: {}\".format(epoch, itr))\n",
        "        print(\"Training loss after epoch {}: {}\".format(epoch, total_loss/len(train_batches)))\n",
        "        test_struct_svm(cnn_model, svm_model, train_batches, solver, x_assign, y1_assign, y2_assign)\n",
        "        test_struct_svm(cnn_model, svm_model, test_batches, solver, x_assign, y1_assign, y2_assign)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBjhoX75rI0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_struct_svm(cnn_model, svm_model, test_batches, solver, x_assign, y1_assign, y2_assign):\n",
        "    # TODO: Write a testing function for the structured SVM\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    class_gold = [0.0] * NUM_CLASSES\n",
        "    class_pred = [0.0] * NUM_CLASSES\n",
        "    class_correct = [0.0] * NUM_CLASSES\n",
        "    total_loss = 0.0\n",
        "    for itr, batch in enumerate(test_batches):\n",
        "        images, labels = batch\n",
        "        fcn_output = cnn_model.get_repr(images)  # returns 1 x 32 x 32 x 100\n",
        "        unary, edge1, edge2 = svm_model(fcn_output)\n",
        "        # Perform MAP inference\n",
        "        unary_np = unary.detach().numpy()\n",
        "        edge1_np = edge1.detach().numpy()\n",
        "        edge2_np = edge2.detach().numpy()\n",
        "        final_x_assign, final_y1_assign, final_y2_assign = map_inference(solver, \n",
        "                                        unary_np, edge1_np, edge2_np, x_assign, y1_assign, y2_assign)\n",
        "        \n",
        "        labels = labels.squeeze()   # Labels = torch.Size([32, 32])\n",
        "        # Construct the Gold X indicators\n",
        "        label_ind = np.zeros((32,32,2))\n",
        "        for i in range(32):\n",
        "          for j in range(32):\n",
        "            label_ind[i,j,labels[i,j]] = 1\n",
        "        # Construct the Gold Y indicators\n",
        "        edge1_ind = torch.zeros((32,31,2))\n",
        "        edge2_ind = torch.zeros((31,32,2))\n",
        "        for i in range(32):\n",
        "          for j in range(31):\n",
        "            if labels[i,j]==1 and labels[i,j+1]==1:\n",
        "              edge1_ind[i,j,1] = 1\n",
        "            if labels[i,j]==0 and labels[i,j+1]==0:\n",
        "              edge1_ind[i,j,0] = 1\n",
        "        for i in range(31):\n",
        "          for j in range(32):\n",
        "            if labels[i,j]==1 and labels[i+1,j]==1:\n",
        "              edge2_ind[i,j,1] = 1\n",
        "            if labels[i,j]==0 and labels[i+1,j]==0:\n",
        "              edge2_ind[i,j,0] = 1\n",
        "        # Structured Hinge Loss\n",
        "        loss = compute_struct_svm_loss(unary, edge1, edge2, label_ind, edge1_ind, edge2_ind,\n",
        "                                           final_x_assign, final_y1_assign, final_y2_assign)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # final_x_assign = torch.Size([32, 32, 2])\n",
        "        output = final_x_assign.view(-1,2)  # torch.Size([1024, 2])\n",
        "        # output = svm_model(fcn_output)       # torch.Size([1024, 2])\n",
        "        _, output = torch.max(output, axis=1)\n",
        "        visualize_grayscale_image(output.view(32, 32).detach().numpy(), itr)\n",
        "        output = output.squeeze().detach().numpy()\n",
        "        labels = labels.contiguous().view(-1, 1).squeeze().numpy()\n",
        "\n",
        "        cur_class_pred = np.unique(output, return_counts=True)\n",
        "        for key, val in zip(cur_class_pred[0], cur_class_pred[1]):\n",
        "            class_pred[key] += val\n",
        "        cur_class_gold = np.unique(labels, return_counts=True)\n",
        "        for key, val in zip(cur_class_gold[0], cur_class_gold[1]):\n",
        "            class_gold[key] += val\n",
        "        cur_correct = (output == labels).tolist()\n",
        "        for j, val in enumerate(cur_correct):\n",
        "            if val:\n",
        "                class_correct[labels[j]] += 1\n",
        "        correct += np.sum(cur_correct)\n",
        "        total += len(labels)\n",
        "        print(\"test\", itr)\n",
        "    class_iou = [x/(y+z-x) for x, y, z in zip(class_correct, class_gold, class_pred)]\n",
        "    mean_iou = sum(class_iou) / len(class_correct)\n",
        "    print(\"Mean IOU: {}\".format(mean_iou))\n",
        "    print(\"Pixel Accuracy: {}\".format(correct / total))\n",
        "    print(\"loss: {}\\n\".format(total_loss/len(test_batches)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gqBg6qhDQm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_inference(solver, unary, edge1, edge2, x_assign, y1_assign, y2_assign):\n",
        "    # Define objective\n",
        "    obj = 0.0\n",
        "    for i in range(32):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          obj += unary[i,j,c]*x_assign[i,j,c]\n",
        "    for i in range(32):\n",
        "      for j in range(31):\n",
        "        for c in range(2):\n",
        "          obj += edge1[i,j,c]*y1_assign[i,j,c]\n",
        "    for i in range(31):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          obj += edge2[i,j,c]*y2_assign[i,j,c]\n",
        "    \n",
        "    solver.Maximize(obj)\n",
        "    \n",
        "    # Solve ILP again\n",
        "    result = solver.Solve()\n",
        "    assert result == pywraplp.Solver.OPTIMAL\n",
        "    assert solver.VerifySolution(1e-7, True)\n",
        "\n",
        "    # Gather the optimal assignment\n",
        "    final_x_assign = torch.zeros((32, 32, 2))\n",
        "    final_y1_assign = torch.zeros((32, 31, 2))\n",
        "    final_y2_assign = torch.zeros((31, 32, 2))\n",
        "    for i in range(32):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          final_x_assign[i, j, c] = x_assign[i,j,c].solution_value()\n",
        "    for i in range(32):\n",
        "      for j in range(31):\n",
        "        for c in range(2):\n",
        "          final_y1_assign[i, j, c] = y1_assign[i,j,c].solution_value()\n",
        "    for i in range(31):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          final_y2_assign[i, j, c] = y2_assign[i,j,c].solution_value()\n",
        "\n",
        "    return final_x_assign, final_y1_assign, final_y2_assign"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsdqzj1UQN6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_grayscale_image(image, file=None):\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    # Uncomment this to visualize image\n",
        "    # plt.show()\n",
        "    # Uncomment this to save image\n",
        "    # plt.savefig(str(file)+\".png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EurepqnDQQbv",
        "colab_type": "code",
        "outputId": "1c20752d-266d-4bae-d477-03b49f3000fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    seed = 42\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Uncomment following lines after providing appropriate paths\n",
        "    path_to_image_folder = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledImages\"\n",
        "    path_to_label_folder = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledLabels\"\n",
        "    file_with_train_ids = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/train.txt\"\n",
        "    file_with_test_ids = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/test.txt\"\n",
        "    \n",
        "    train_dataset = VOCDataset(path_to_image_folder, path_to_label_folder, file_with_train_ids)\n",
        "    test_dataset = VOCDataset(path_to_image_folder, path_to_label_folder, file_with_test_ids)\n",
        "\n",
        "    train_batches = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
        "    test_batches = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    cnn = SimpleSegmentationModel(NUM_FEATURES, NUM_CLASSES)\n",
        "    train_cnn(cnn, train_batches, test_batches, 2)\n",
        "    test_cnn(cnn, test_batches)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss after epoch 0: 0.6317094841484349\n",
            "Mean IOU: 0.44119045060334416\n",
            "Pixel Accuracy: 0.66916325777897\n",
            "\n",
            "Training loss after epoch 1: 0.6292329457440602\n",
            "Mean IOU: 0.4418289920934885\n",
            "Pixel Accuracy: 0.6704927239806867\n",
            "\n",
            "Mean IOU: 0.4312953923109335\n",
            "Pixel Accuracy: 0.6629033554888508\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npm21YPgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnsRxJ2DoN69",
        "colab_type": "code",
        "outputId": "ae00b968-65b6-481b-913e-7d550eb2d276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        }
      },
      "source": [
        "# TODO: Instantiate a linear SVM and call train/ test functions\n",
        "linear_svm = LinearSVM(n_feat = NUM_FEATURES, n_classes = NUM_CLASSES)\n",
        "train_linear_svm(cnn, linear_svm, train_batches, test_batches, num_epochs = 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss after epoch 0: 0.6521463360653414\n",
            "Mean IOU: 0.4101404705889763\n",
            "Pixel Accuracy: 0.6203422579130902\n",
            "loss: 0.6287332274179602\n",
            "\n",
            "Mean IOU: 0.39894779255127805\n",
            "Pixel Accuracy: 0.6109478853987993\n",
            "loss: 0.6435440161215912\n",
            "\n",
            "Training loss after epoch 1: 0.6280096775792188\n",
            "Mean IOU: 0.40740745546090273\n",
            "Pixel Accuracy: 0.6165835401019313\n",
            "loss: 0.6273418982269426\n",
            "\n",
            "Mean IOU: 0.3960251468261866\n",
            "Pixel Accuracy: 0.6068356024871355\n",
            "loss: 0.6411339949886754\n",
            "\n",
            "Training loss after epoch 2: 0.6264184534997388\n",
            "Mean IOU: 0.40758702734701396\n",
            "Pixel Accuracy: 0.6169008181330472\n",
            "loss: 0.6263728263488143\n",
            "\n",
            "Mean IOU: 0.3961840186465297\n",
            "Pixel Accuracy: 0.6071806657375644\n",
            "loss: 0.6403504349355224\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq_wDnXtwFKb",
        "colab_type": "code",
        "outputId": "6215d5a8-b7f1-464c-8788-2a57dfcdbb49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TODO: Instantiate a structured SVM and call train/ test functions\n",
        "struct_svm = StructSVM(n_feat = NUM_FEATURES, n_classes = NUM_CLASSES, w = 32, h = 32)\n",
        "train_struct_svm(cnn, struct_svm, train_batches, test_batches, num_epochs = 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss after epoch 0: 16.311788727956436\n",
            "Mean IOU: 0.39651594338070223\n",
            "Pixel Accuracy: 0.6926724282457082\n",
            "loss: 7.541750848996393\n",
            "\n",
            "Mean IOU: 0.4032909111422687\n",
            "Pixel Accuracy: 0.6979490512435678\n",
            "loss: 7.5559381893133075\n",
            "\n",
            "Training loss after epoch 1: 6.571665256368516\n",
            "Mean IOU: 0.4397148492698352\n",
            "Pixel Accuracy: 0.7376722605954935\n",
            "loss: 5.693616106192073\n",
            "\n",
            "Mean IOU: 0.4358472837830808\n",
            "Pixel Accuracy: 0.7425811401157805\n",
            "loss: 5.99959721065751\n",
            "\n",
            "Training loss after epoch 2: 5.064160602259182\n",
            ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-74713f4ad830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstruct_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStructSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUM_FEATURES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_struct_svm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-68-5d4a63fb906f>\u001b[0m in \u001b[0;36mtrain_struct_svm\u001b[0;34m(cnn_model, svm_model, train_batches, test_batches, num_epochs)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {}, itr: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training loss after epoch {}: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mtest_struct_svm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_assign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_assign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_assign\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mtest_struct_svm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_assign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_assign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_assign\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-4f382e6bc63a>\u001b[0m in \u001b[0;36mtest_struct_svm\u001b[0;34m(cnn_model, svm_model, test_batches, solver, x_assign, y1_assign, y2_assign)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0medge2_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         final_x_assign, final_y1_assign, final_y2_assign = map_inference(solver, \n\u001b[0;32m---> 18\u001b[0;31m                                         unary_np, edge1_np, edge2_np, x_assign, y1_assign, y2_assign)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Labels = torch.Size([32, 32])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-673bfa453e92>\u001b[0m in \u001b[0;36mmap_inference\u001b[0;34m(solver, unary, edge1, edge2, x_assign, y1_assign, y2_assign)\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0mobj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0medge2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my2_assign\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Solve ILP again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ortools/linear_solver/pywraplp.py\u001b[0m in \u001b[0;36mMaximize\u001b[0;34m(self, expr)\u001b[0m\n\u001b[1;32m    304\u001b[0m           \u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetOffset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m           \u001b[0mcoeffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetCoeffs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m           \u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetOffset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOFFSET_KEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ortools/linear_solver/linear_solver_natural_api.py\u001b[0m in \u001b[0;36mGetCoeffs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mcurrent_multiplier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_expression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m       current_expression.AddSelfToCoeffMapOrStack(coeffs, current_multiplier,\n\u001b[0;32m--> 101\u001b[0;31m                                                   stack)\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcoeffs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ortools/linear_solver/linear_solver_natural_api.py\u001b[0m in \u001b[0;36mAddSelfToCoeffMapOrStack\u001b[0;34m(self, coeffs, multiplier, stack)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;31m# predictable from user perspective.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m       \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiplier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzeG8AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frw2RAcsI9uU",
        "colab_type": "code",
        "outputId": "8c4d696c-f946-4e2a-bd1e-68ab59e34567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "solver = pywraplp.Solver('LinearExample', pywraplp.Solver.GLOP_LINEAR_PROGRAMMING)\n",
        "# Define assignment variables\n",
        "x_assign = {}\n",
        "for i in range(32):\n",
        "  for j in range(32):\n",
        "    for c in range(2):\n",
        "      x_assign[i, j, c] = solver.NumVar(0.0, 1.0, 'x[%d,%d, %d]' % (i, j, c))\n",
        "# Define assignment variables\n",
        "y1_assign = {}\n",
        "for i in range(32):\n",
        "  for j in range(31):\n",
        "    for c in range(2):\n",
        "      y1_assign[i, j, c] = solver.NumVar(0.0, 1.0, 'y1[%d,%d, %d]' % (i, j, c))\n",
        "# Define assignment variables\n",
        "y2_assign = {}\n",
        "for i in range(31):\n",
        "  for j in range(32):\n",
        "    for c in range(2):\n",
        "      y2_assign[i, j, c] = solver.NumVar(0.0, 1.0, 'y2[%d,%d, %d]' % (i, j, c))\n",
        "# Add constraints\n",
        "for i in range(32):\n",
        "  for j in range(32):\n",
        "    solver.Add(solver.Sum([x_assign[i,j,c] for c in range(2)])==1)\n",
        "# Add constraints\n",
        "for i in range(32):\n",
        "  for j in range(31):\n",
        "    for c in range(2):\n",
        "      solver.Add(y1_assign[i,j,c]<=x_assign[i,j,c])\n",
        "      solver.Add(y1_assign[i,j,c]<=x_assign[i,j+1,c])\n",
        "# Add constraints\n",
        "for i in range(31):\n",
        "  for j in range(32):\n",
        "    for c in range(2):\n",
        "      solver.Add(y2_assign[i,j,c]<=x_assign[i,j,c])\n",
        "      solver.Add(y2_assign[i,j,c]<=x_assign[i+1,j,c])\n",
        "\n",
        "test_struct_svm(cnn, struct_svm, test_batches, solver, x_assign, y1_assign, y2_assign)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean IOU: 0.4303872459999149\n",
            "Pixel Accuracy: 0.764484281196398\n",
            "loss: 4.659128742004979\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n43Zu6EDxlIcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2z6ezf1TaCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
