{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10618_HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arndmghsh/ImageSegmentation-StructuredSVM/blob/master/10618_HW3_2_linearSVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDKtlPYoRAqi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "2fcd1be5-5d54-490f-b0a1-24420ee2020e"
      },
      "source": [
        "# !pip install --upgrade ortools"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ortools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/c2/81a18b1ee4e9c8401fa2c9bc6fcd1b59b1c30ff7a1b214aff84d718d95c5/ortools-7.4.7247-cp36-cp36m-manylinux1_x86_64.whl (27.7MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7MB 88kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from ortools) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.10.0 in /usr/local/lib/python3.6/dist-packages (from ortools) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.10.0->ortools) (41.4.0)\n",
            "Installing collected packages: ortools\n",
            "Successfully installed ortools-7.4.7247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkXXNvkBekZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnNyEsQ6hJzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip -uq \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledImages.zip\" -d \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledImages\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQkZPu-1j3cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip -uq \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledLabels.zip\" -d \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zxfCW9PO7oXB",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from ortools.linear_solver import pywraplp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4yT4lrdDyKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_CLASSES = 2\n",
        "NUM_FEATURES = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irbTR7XvPTs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VOCDataset(Dataset):\n",
        "    \"\"\"Class to store VOC semantic segmentation dataset\"\"\"\n",
        "\n",
        "    def __init__(self, image_dir, label_dir, file_list):\n",
        "\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        reader = open(file_list, \"r\")\n",
        "        self.files = []\n",
        "        for file in reader:\n",
        "            self.files.append(file.strip())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.files[idx]\n",
        "        # 0 stands for background, 1 for foreground\n",
        "        labels = np.load(os.path.join(self.label_dir, fname+\".npy\"))\n",
        "        labels[labels > 0.0] = 1.0\n",
        "        image = Image.open(os.path.join(self.image_dir, fname+\".jpg\"), \"r\")\n",
        "        sample = (TF.to_tensor(image), torch.LongTensor(labels))\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mv1YMAzPWN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "    \"\"\"Class defining AlexNet layers used for the convolutional network\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=2, padding=4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9umKmhR1Padf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FCNHead(nn.Sequential):\n",
        "    \"\"\"Class defining FCN (fully convolutional network) layers\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, channels):\n",
        "        inter_channels = in_channels // 4\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(inter_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv2d(inter_channels, channels, 1)\n",
        "        ]\n",
        "\n",
        "        super(FCNHead, self).__init__(*layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRX-E4THPjBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleSegmentationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Class defining end-to-end semantic segmentation model.\n",
        "    It combines AlexNet and FCN layers with interpolation for deconvolution.\n",
        "    This model is pretrained using cross-entropy loss.\n",
        "    After pre-training, use the get_repr() function to construct 32x32x100 feature tensors for each image\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_feat, n_classes):\n",
        "        super(SimpleSegmentationModel, self).__init__()\n",
        "        self.n_feat = n_feat\n",
        "        self.backbone = AlexNet()\n",
        "        self.classifier = FCNHead(256, n_feat)\n",
        "        self.linear = nn.Linear(n_feat, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_shape = x.shape[-2:]\n",
        "        features = self.backbone(x)\n",
        "        x = self.classifier(features)\n",
        "        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous().view(-1, self.n_feat)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_repr(self, x):\n",
        "        input_shape = x.shape[-2:]\n",
        "        features = self.backbone(x)\n",
        "        x = self.classifier(features)\n",
        "        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxiDrt8aPvcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearSVM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_feat, n_classes):\n",
        "        super(LinearSVM, self).__init__()\n",
        "        self.n_feat = n_feat\n",
        "        self.n_classes = n_classes\n",
        "        # TODO: Define weights for linear SVM\n",
        "        # self.w = torch.autograd.Variable(torch.rand(n_feat), requires_grad=True)\n",
        "        # self.b = torch.autograd.Variable(torch.rand(1), requires_grad=True)\n",
        "        self.linear = nn.Linear(n_feat, n_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # TODO: Define forward function for linear SVM\n",
        "        # x: 1 x 32 x 32 x 100, w: 100 , b: 1\n",
        "        # y_hat = torch.dot(x,self.w)+self.b\n",
        "        x = x.contiguous().view(-1, self.n_feat)\n",
        "        # x: 1024 x 100\n",
        "        y_hat = self.linear(x)      # returns 1024 x 2\n",
        "        return y_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djSlxCHmP0n4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StructSVM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_feat, n_classes, w, h):\n",
        "        super(StructSVM, self).__init__()\n",
        "        self.n_feat = n_feat\n",
        "        self.n_classes = n_classes\n",
        "        self.w = w\n",
        "        self.h = h\n",
        "\n",
        "        # TODO: Define weights for structured SVM\n",
        "\n",
        "\n",
        "    def forward(self, image):\n",
        "        # TODO: Define forward function for structured SVM\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rljo5bqJP3NK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_cnn(model, train_batches, test_batches, num_epochs):\n",
        "    \"\"\"\n",
        "    This function runs a training loop for the FCN semantic segmentation model\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss(weight=torch.Tensor([1, 4]))\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for i, batch in enumerate(train_batches):\n",
        "            optimizer.zero_grad()\n",
        "            images, labels = batch\n",
        "            # Images: torch.Size([1, 3, 32, 32]), Labels = torch.Size([1, 32, 32])\n",
        "            output = model(images)\n",
        "            # output: 32*32 x 2 = torch.Size([1024, 2])\n",
        "            labels = labels.contiguous().view(-1, 1).squeeze()\n",
        "            # labels: torch.Size([1024])\n",
        "            loss = criterion(output, labels)  # inputs: torch.Size([1024, 2]) and torch.Size([1024])\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # print(\"Epoch: {}, itr: {}\".format(epoch, i))\n",
        "        print(\"Training loss after epoch {}: {}\".format(epoch, total_loss/len(train_batches)))\n",
        "        test_cnn(model, train_batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcUqpbKxPnH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_cnn(model, test_batches):\n",
        "    \"\"\"\n",
        "        This function evaluates the FCN semantic segmentation model on the test set\n",
        "    \"\"\"\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    class_gold = [0.0] * NUM_CLASSES\n",
        "    class_pred = [0.0] * NUM_CLASSES\n",
        "    class_correct = [0.0] * NUM_CLASSES\n",
        "    for i, batch in enumerate(test_batches):\n",
        "        images, labels = batch\n",
        "        output = model(images)\n",
        "        _, output = torch.max(output, axis=1)\n",
        "        visualize_grayscale_image(output.view(32, 32).detach().numpy(), i)\n",
        "        output = output.squeeze().detach().numpy()\n",
        "        labels = labels.contiguous().view(-1, 1).squeeze().numpy()\n",
        "        cur_class_pred = np.unique(output, return_counts=True)\n",
        "        for key, val in zip(cur_class_pred[0], cur_class_pred[1]):\n",
        "            class_pred[key] += val\n",
        "        cur_class_gold = np.unique(labels, return_counts=True)\n",
        "        for key, val in zip(cur_class_gold[0], cur_class_gold[1]):\n",
        "            class_gold[key] += val\n",
        "        cur_correct = (output == labels).tolist()\n",
        "        for j, val in enumerate(cur_correct):\n",
        "            if val:\n",
        "                class_correct[labels[j]] += 1\n",
        "        correct += np.sum(cur_correct)\n",
        "        total += len(labels)\n",
        "    class_iou = [x/(y+z-x) for x, y, z in zip(class_correct, class_gold, class_pred)]\n",
        "    mean_iou = sum(class_iou) / len(class_correct)\n",
        "    print(\"Mean IOU: {}\".format(mean_iou))\n",
        "    print(\"Pixel Accuracy: {}\\n\".format(correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud2IwN-kP97o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_linear_svm(cnn_model, svm_model, train_batches, test_batches, num_epochs):\n",
        "    # TODO: Write a training loop for the linear SVM\n",
        "    # Keep in mind that the CNN model is needed to compute features, but it should not be finetuned\n",
        "    criterion = nn.MultiMarginLoss(weight=torch.Tensor([1, 4]))  # Class weights to handle class imbalance\n",
        "    optimizer = optim.Adam(svm_model.parameters(), lr=0.0001)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for i, batch in enumerate(train_batches):\n",
        "            optimizer.zero_grad()\n",
        "            images, labels = batch\n",
        "            # Images: torch.Size([1, 3, 32, 32]), Labels = torch.Size([1, 32, 32])\n",
        "            fcn_output = cnn_model.get_repr(images)  # returns 1 x 32 x 32 x 100\n",
        "            output = svm_model(fcn_output)       # torch.Size([1024, 2])\n",
        "            labels = labels.contiguous().view(-1, 1).squeeze()   # labels: torch.Size([1024])\n",
        "            loss = criterion(output, labels)    # inputs = torch.Size([1024, 2]) and torch.Size([1024])\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # print(\"Epoch: {}, itr: {}\".format(epoch, i))\n",
        "        print(\"Training loss after epoch {}: {}\".format(epoch, total_loss/len(train_batches)))\n",
        "        test_linear_svm(cnn_model, svm_model, train_batches)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDHSYvVUQAt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_linear_svm(cnn_model, svm_model, test_batches):\n",
        "    # TODO: Write a testing function for the linear SVM\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    class_gold = [0.0] * NUM_CLASSES\n",
        "    class_pred = [0.0] * NUM_CLASSES\n",
        "    class_correct = [0.0] * NUM_CLASSES\n",
        "    for i, batch in enumerate(test_batches):\n",
        "        images, labels = batch\n",
        "        fcn_output = cnn_model.get_repr(images)  # returns 1 x 32 x 32 x 100\n",
        "        output = svm_model(fcn_output)       # torch.Size([1024, 2])\n",
        "        _, output = torch.max(output, axis=1)\n",
        "        visualize_grayscale_image(output.view(32, 32).detach().numpy(), i)\n",
        "        output = output.squeeze().detach().numpy()\n",
        "        labels = labels.contiguous().view(-1, 1).squeeze().numpy()\n",
        "\n",
        "        cur_class_pred = np.unique(output, return_counts=True)\n",
        "        for key, val in zip(cur_class_pred[0], cur_class_pred[1]):\n",
        "            class_pred[key] += val\n",
        "        cur_class_gold = np.unique(labels, return_counts=True)\n",
        "        for key, val in zip(cur_class_gold[0], cur_class_gold[1]):\n",
        "            class_gold[key] += val\n",
        "        cur_correct = (output == labels).tolist()\n",
        "        for j, val in enumerate(cur_correct):\n",
        "            if val:\n",
        "                class_correct[labels[j]] += 1\n",
        "        correct += np.sum(cur_correct)\n",
        "        total += len(labels)\n",
        "    class_iou = [x/(y+z-x) for x, y, z in zip(class_correct, class_gold, class_pred)]\n",
        "    mean_iou = sum(class_iou) / len(class_correct)\n",
        "    print(\"Mean IOU: {}\".format(mean_iou))\n",
        "    print(\"Pixel Accuracy: {}\\n\".format(correct / total))\n",
        "\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXlETq3tQDBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Write a function to compute the structured hinge loss\n",
        "# using the max-scoring output from the ILP and the gold output\n",
        "# def compute_struct_svm_loss():\n",
        "#     return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIHVXtM8QGMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_struct_svm(cnn_model, svm_model, train_batches, test_batches, num_epochs):\n",
        "    # TODO: Write a training loop for the structured SVM\n",
        "    # Keep in mind that the CNN model is needed to compute features, but it should not be finetuned\n",
        "    return\n",
        "\n",
        "\n",
        "def test_struct_svm(cnn_model, svm_model, test_batches):\n",
        "    # TODO: Write a testing function for the structured SVM\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsdqzj1UQN6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_grayscale_image(image, file=None):\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    # Uncomment this to visualize image\n",
        "    # plt.show()\n",
        "    # Uncomment this to save image\n",
        "    # plt.savefig(str(file)+\".png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EurepqnDQQbv",
        "colab_type": "code",
        "outputId": "dac6a5b8-9401-4889-d8e0-2dd5c2ea2318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    seed = 42\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Uncomment following lines after providing appropriate paths\n",
        "    path_to_image_folder = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledImages\"\n",
        "    path_to_label_folder = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledLabels\"\n",
        "    file_with_train_ids = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/train.txt\"\n",
        "    file_with_test_ids = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/test.txt\"\n",
        "    \n",
        "    train_dataset = VOCDataset(path_to_image_folder, path_to_label_folder, file_with_train_ids)\n",
        "    test_dataset = VOCDataset(path_to_image_folder, path_to_label_folder, file_with_test_ids)\n",
        "\n",
        "    train_batches = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "    test_batches = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    cnn = SimpleSegmentationModel(NUM_FEATURES, NUM_CLASSES)\n",
        "    train_cnn(cnn, train_batches, test_batches, 2)\n",
        "    test_cnn(cnn, test_batches)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss after epoch 0: 0.6329919149819362\n",
            "Mean IOU: 0.4555737758355717\n",
            "Pixel Accuracy: 0.6954441054184549\n",
            "\n",
            "Training loss after epoch 1: 0.6290672038118215\n",
            "Mean IOU: 0.45211469879939115\n",
            "Pixel Accuracy: 0.6889015557939914\n",
            "\n",
            "Mean IOU: 0.4426207415716136\n",
            "Pixel Accuracy: 0.6826138373713551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnsRxJ2DoN69",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "af067286-ab6e-4db9-b417-dd3371344287"
      },
      "source": [
        "# TODO: Instantiate a linear SVM and call train/ test functions\n",
        "linear_svm = LinearSVM(n_feat = NUM_FEATURES, n_classes = NUM_CLASSES)\n",
        "train_linear_svm(cnn, linear_svm, train_batches, test_batches, num_epochs = 3)\n",
        "test_linear_svm(cnn, linear_svm, test_batches)\n",
        "# TODO: Instantiate a structured SVM and call train/ test functions\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss after epoch 0: 0.6509752579640933\n",
            "Mean IOU: 0.3989965572194325\n",
            "Pixel Accuracy: 0.6033462982832618\n",
            "\n",
            "Training loss after epoch 1: 0.6299050095894818\n",
            "Mean IOU: 0.39956837888217933\n",
            "Pixel Accuracy: 0.6042972941255365\n",
            "\n",
            "Training loss after epoch 2: 0.6291903404207189\n",
            "Mean IOU: 0.4011234037881908\n",
            "Pixel Accuracy: 0.6067554653969958\n",
            "\n",
            "Mean IOU: 0.38932917204176226\n",
            "Pixel Accuracy: 0.596430102915952\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAMFUlEQVR4nO3db6hk9X3H8fenRtsShWhtl2U1NVpp\nCSFVEUlBgg0kWJ+oUMRAwULghlJBHxQqKTS2j5ISDX1ksVUipTW1takipcaKJXlkXO26rm4TNShx\nWV2CDeqTpMZvH8xZuLvsvXf2zpk/937fLxjmzJm553w53M/8fufMzO+XqkLS7vcLyy5A0mIYdqkJ\nwy41YdilJgy71IRhl5r40Cx/nORa4K+BM4C/q6qvbPF6P+eT5qyqcqr12e7n7EnOAH4AfBZ4A3gG\n+HxVvbTJ3xh2ac42Cvss3firgFeq6odV9TPgm8D1M2xP0hzNEvZ9wI/WPX5jWCdpBc10zj6NJGvA\n2rz3I2lzs4T9CHDhuscXDOtOUFX3AveC5+zSMs3SjX8GuDTJx5KcBdwMPDpOWZLGtu2WvareT3Ir\n8DiTj97ur6oXR6tM0qi2/dHbtnZmN16au3l89CZpBzHsUhOGXWrCsEtNGHapCcMuNWHYpSYMu9SE\nYZeaMOxSE4ZdasKwS00YdqkJwy41YdilJgy71IRhl5ow7FIThl1qwrBLTRh2qQnDLjVh2KUmDLvU\nhGGXmphpFtckrwHvAj8H3q+qK8coStL4xpiy+Xer6scjbEfSHNmNl5qYNewFfDvJs0nWxihI0nzM\n2o2/uqqOJPk14Ikk/1NV31n/guFNwDcCaclGm7I5yZ3Ae1X1tU1e45TN0pyNPmVzkg8nOef4MvA5\n4NB2tydpvmbpxu8BvpXk+Hb+sar+Y5SqVtRYvSCttuF/etcZrRs/1c52eDfesPew08M+ejde0s5i\n2KUmDLvUhGGXmjDsUhNj/BBmx/Gqujaz3f+PVb+Kb8suNWHYpSYMu9SEYZeaMOxSE4ZdasKwS00Y\ndqkJwy41YdilJgy71IRhl5ow7FIThl1qwrBLTRh2qQnDLjVh2KUmDLvUxJZhT3J/kmNJDq1bd16S\nJ5K8PNyfO98yJc1qmpb9G8C1J627A3iyqi4FnhweS1phW4Z9mG/97ZNWXw88MCw/ANwwcl2SRrbd\nc/Y9VXV0WH6TyYyuklbYzOPGV1VtNjtrkjVgbdb9SJrNdlv2t5LsBRjuj230wqq6t6qurKort7kv\nSSPYbtgfBW4Zlm8BHhmnHEnzkq2muknyIHANcD7wFvBl4N+Ah4CPAq8DN1XVyRfxTrWtlZh3yemf\nNA+rMv1TVZ2ykC3DPibDrt1s1cPuN+ikJgy71IRhl5ow7FIThl1qwrBLTRh2qQnDLjVh2KUmDLvU\nhGGXmjDsUhMzD16xqvyxi3QiW3apCcMuNWHYpSYMu9SEYZea2LVX4zcbIsgr9erIll1qwrBLTRh2\nqQnDLjVh2KUmDLvUxJZhT3J/kmNJDq1bd2eSI0kODLfr5lumpFlN07J/A7j2FOu/XlWXDbd/H7cs\nSWPbMuxV9R1gy0kbJa22Wc7Zb01ycOjmnztaRZLmYrthvwe4BLgMOArctdELk6wl2Z9k/zb3JWkE\nU03ZnOQi4LGq+sTpPHeK167El9L9brzmYVdO2Zxk77qHNwKHNnqtpNWw5a/ekjwIXAOcn+QN4MvA\nNUkuAwp4DfjiHGuUNIKpuvGj7cxuvHaxXdmNl7TzGHapCcMuNWHYpSYMu9SEYZeaMOxSE4ZdasKw\nS00YdqkJwy41YdilJnbtXG/Som32A6tV+JGMLbvUhGGXmjDsUhOGXWrCsEtNGHapCcMuNWHYpSYM\nu9SEYZeaMOxSE4ZdamLLsCe5MMlTSV5K8mKS24b15yV5IsnLw73TNksrbMvpn4ZJHPdW1XNJzgGe\nBW4A/hB4u6q+kuQO4Nyq+tMttrUS8y45/ZMWbZG/etv29E9VdbSqnhuW3wUOA/uA64EHhpc9wOQN\nQNKKOq1z9mEu9suBp4E9VXV0eOpNYM+olUka1dSDVyQ5G3gYuL2q3lnfLamq2qiLnmQNWJu1UEmz\nmWrK5iRnAo8Bj1fV3cO67wPXVNXR4bz+v6rqN7fYzkqcLHvOrkXbEefsmVR5H3D4eNAHjwK3DMu3\nAI/MWqSk+ZnmavzVwHeBF4APhtVfYnLe/hDwUeB14KaqenuLba1Ek2rLrkVbhZZ9qm78WAy7ulqF\nsPsNOqkJwy41YdilJgy71IRhl5rYtdM/ecVdOpEtu9SEYZeaMOxSE4ZdasKwS00YdqkJwy41Ydil\nJgy71IRhl5ow7FIThl1qYtf+EGazYYD8kYzmYZFDT22HLbvUhGGXmjDsUhOGXWrCsEtNGHapiWnm\nerswyVNJXkryYpLbhvV3JjmS5MBwu27+5Y4jyYY3aTM7+X9nmrne9gJ7q+q5JOcAzwI3ADcB71XV\n16be2YpM/7QZP4PXZnZCqDea/mnLL9VU1VHg6LD8bpLDwL5xy5M0b6d1zp7kIuByJjO4Atya5GCS\n+5OcO3JtkkY0ddiTnA08DNxeVe8A9wCXAJcxafnv2uDv1pLsT7J/hHolbdNUUzYnORN4DHi8qu4+\nxfMXAY9V1Se22M7KnxB7zq7N7ORz9mmuxge4Dzi8PujDhbvjbgQOzVqkpPmZ5mr81cB3gReAD4bV\nXwI+z6QLX8BrwBeHi3mbbWtHN5s7odXfqOXZCbUv0k5oobdro5Z9qm78WAz7/Bn26XQMu9+gk5ow\n7FIThl1qwrBLTRh2qYldO+DkPOzkK7g7uXaNw5ZdasKwS00YdqkJwy41YdilJgy71IRhl5ow7FIT\nhl1qwrBLTRh2qQnDLjVh2KUmDLvUhGGXmjDsUhOGXWrCsEtNGHapiWnmevulJN9L8nySF5P8xbD+\nY0meTvJKkn9Kctb8y5W0XdO07D8FPlNVv81kbrdrk3wK+Crw9ar6DeB/gS/Mr0xJs9oy7DXx3vDw\nzOFWwGeAfxnWPwDcMJcKJY1iqnP2JGckOQAcA54AXgV+UlXvDy95A9g3nxIljWGqsFfVz6vqMuAC\n4Crgt6bdQZK1JPuT7N9mjZJGcFpX46vqJ8BTwO8AH0lyfJKJC4AjG/zNvVV1ZVVdOVOlkmYyzdX4\nX03ykWH5l4HPAoeZhP73h5fdAjwyryIlzS5VtfkLkk8yuQB3BpM3h4eq6i+TXAx8EzgP+G/gD6rq\np1tsa/OdSZpZVZ1yrq8twz4mwy7N30Zh9xt0UhOGXWrCsEtNGHapCcMuNfGhrV8yqh8Drw/L5w+P\nl806TmQdJ9ppdfz6Rk8s9KO3E3ac7F+Fb9VZh3V0qcNuvNSEYZeaWGbY713ivtezjhNZx4l2TR1L\nO2eXtFh246UmlhL2JNcm+f4wWOUdy6hhqOO1JC8kObDIwTWS3J/kWJJD69adl+SJJC8P9+cuqY47\nkxwZjsmBJNctoI4LkzyV5KVhUNPbhvULPSab1LHQYzK3QV6raqE3Jj+VfRW4GDgLeB74+KLrGGp5\nDTh/Cfv9NHAFcGjdur8C7hiW7wC+uqQ67gT+ZMHHYy9wxbB8DvAD4OOLPiab1LHQYwIEOHtYPhN4\nGvgU8BBw87D+b4A/Op3tLqNlvwp4pap+WFU/Y/Kb+OuXUMfSVNV3gLdPWn09k3EDYEEDeG5Qx8JV\n1dGqem5YfpfJ4Cj7WPAx2aSOhaqJ0Qd5XUbY9wE/Wvd4mYNVFvDtJM8mWVtSDcftqaqjw/KbwJ4l\n1nJrkoNDN3/upxPrJbkIuJxJa7a0Y3JSHbDgYzKPQV67X6C7uqquAH4P+OMkn152QTB5Z2fyRrQM\n9wCXMJkj4Chw16J2nORs4GHg9qp6Z/1zizwmp6hj4cekZhjkdSPLCPsR4MJ1jzccrHLequrIcH8M\n+BaTg7osbyXZCzDcH1tGEVX11vCP9gHwtyzomCQ5k0nA/qGq/nVYvfBjcqo6lnVMhn2f9iCvG1lG\n2J8BLh2uLJ4F3Aw8uugiknw4yTnHl4HPAYc2/6u5epTJwJ2wxAE8j4drcCMLOCZJAtwHHK6qu9c9\ntdBjslEdiz4mcxvkdVFXGE+62ngdkyudrwJ/tqQaLmbyScDzwIuLrAN4kEl38P+YnHt9AfgV4Eng\nZeA/gfOWVMffAy8AB5mEbe8C6riaSRf9IHBguF236GOySR0LPSbAJ5kM4nqQyRvLn6/7n/0e8Arw\nz8Avns52/Qad1ET3C3RSG4ZdasKwS00YdqkJwy41YdilJgy71IRhl5r4f7aeUUYiVkHzAAAAAElF\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}