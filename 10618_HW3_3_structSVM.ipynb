{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10618_HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arndmghsh/ImageSegmentation-StructuredSVM/blob/master/10618_HW3_3_structSVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDKtlPYoRAqi",
        "colab_type": "code",
        "outputId": "5c8721bc-4469-4b87-9490-66a0e790b492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip install --upgrade ortools"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ortools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/c2/81a18b1ee4e9c8401fa2c9bc6fcd1b59b1c30ff7a1b214aff84d718d95c5/ortools-7.4.7247-cp36-cp36m-manylinux1_x86_64.whl (27.7MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7MB 81kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from ortools) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.10.0 in /usr/local/lib/python3.6/dist-packages (from ortools) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.10.0->ortools) (41.4.0)\n",
            "Installing collected packages: ortools\n",
            "Successfully installed ortools-7.4.7247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkXXNvkBekZT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c3ae8f91-ea1b-4635-c435-d056aba378e3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnNyEsQ6hJzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip -uq \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledImages.zip\" -d \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledImages\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQkZPu-1j3cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip -uq \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledLabels.zip\" -d \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zxfCW9PO7oXB",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from ortools.linear_solver import pywraplp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4yT4lrdDyKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_CLASSES = 2\n",
        "NUM_FEATURES = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irbTR7XvPTs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VOCDataset(Dataset):\n",
        "    \"\"\"Class to store VOC semantic segmentation dataset\"\"\"\n",
        "\n",
        "    def __init__(self, image_dir, label_dir, file_list):\n",
        "\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        reader = open(file_list, \"r\")\n",
        "        self.files = []\n",
        "        for file in reader:\n",
        "            self.files.append(file.strip())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.files[idx]\n",
        "        # 0 stands for background, 1 for foreground\n",
        "        labels = np.load(os.path.join(self.label_dir, fname+\".npy\"))\n",
        "        labels[labels > 0.0] = 1.0\n",
        "        image = Image.open(os.path.join(self.image_dir, fname+\".jpg\"), \"r\")\n",
        "        sample = (TF.to_tensor(image), torch.LongTensor(labels))\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mv1YMAzPWN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "    \"\"\"Class defining AlexNet layers used for the convolutional network\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=2, padding=4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9umKmhR1Padf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FCNHead(nn.Sequential):\n",
        "    \"\"\"Class defining FCN (fully convolutional network) layers\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, channels):\n",
        "        inter_channels = in_channels // 4\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(inter_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv2d(inter_channels, channels, 1)\n",
        "        ]\n",
        "\n",
        "        super(FCNHead, self).__init__(*layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRX-E4THPjBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleSegmentationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Class defining end-to-end semantic segmentation model.\n",
        "    It combines AlexNet and FCN layers with interpolation for deconvolution.\n",
        "    This model is pretrained using cross-entropy loss.\n",
        "    After pre-training, use the get_repr() function to construct 32x32x100 feature tensors for each image\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_feat, n_classes):\n",
        "        super(SimpleSegmentationModel, self).__init__()\n",
        "        self.n_feat = n_feat\n",
        "        self.backbone = AlexNet()\n",
        "        self.classifier = FCNHead(256, n_feat)\n",
        "        self.linear = nn.Linear(n_feat, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_shape = x.shape[-2:]\n",
        "        features = self.backbone(x)\n",
        "        x = self.classifier(features)\n",
        "        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous().view(-1, self.n_feat)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_repr(self, x):\n",
        "        input_shape = x.shape[-2:]\n",
        "        features = self.backbone(x)\n",
        "        x = self.classifier(features)\n",
        "        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxiDrt8aPvcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearSVM(nn.Module):\n",
        "    def __init__(self, n_feat, n_classes):\n",
        "        super(LinearSVM, self).__init__()\n",
        "        self.n_feat = n_feat\n",
        "        self.n_classes = n_classes\n",
        "        # TODO: Define weights for linear SVM\n",
        "        # self.w = torch.autograd.Variable(torch.rand(n_feat), requires_grad=True)\n",
        "        # self.b = torch.autograd.Variable(torch.rand(1), requires_grad=True)\n",
        "        self.linear = nn.Linear(n_feat, n_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # TODO: Define forward function for linear SVM\n",
        "        # x: 1 x 32 x 32 x 100\n",
        "        x = x.contiguous().view(-1, self.n_feat)\n",
        "        # x: 1024 x 100\n",
        "        y_hat = self.linear(x)      # returns 1024 x 2\n",
        "        return y_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djSlxCHmP0n4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StructSVM(nn.Module):\n",
        "    def __init__(self, n_feat, n_classes, w, h):\n",
        "        super(StructSVM, self).__init__()\n",
        "        self.n_feat = n_feat\n",
        "        self.n_classes = n_classes\n",
        "        self.w = w\n",
        "        self.h = h\n",
        "        # TODO: Define weights for structured SVM\n",
        "        self.lin100 = nn.Linear(n_feat, n_classes)\n",
        "        self.lin200 = nn.Linear(2*n_feat, n_classes)\n",
        "\n",
        "    def forward(self, image):\n",
        "        # TODO: Define forward function for structured SVM\n",
        "        # image: 1 x 32 x 32 x 100\n",
        "        image = image.squeeze() # torch.Size([32, 32, 100])\n",
        "        unary = self.lin100(image)  # torch.Size([32, 32, 2])\n",
        "        # [i,j] and [i,j+1]\n",
        "        edge1 = torch.zeros(self.w, self.h-1, 2)\n",
        "        for i in range(self.w):\n",
        "          for j in range(self.h-1):\n",
        "            # image[i,j].shape, image[i,j+1].shape =  torch.Size([100])\n",
        "            concat = torch.cat((image[i,j], image[i,j+1]), dim=0)\n",
        "            # concat shape = torch.Size([200])\n",
        "            edge1[i,j] = self.lin200(concat)\n",
        "        # [i,j] and [i+1,j]\n",
        "        edge2 = torch.zeros(self.w-1, self.h, 2)\n",
        "        for i in range(self.w-1):\n",
        "          for j in range(self.h):\n",
        "            concat = torch.cat((image[i,j], image[i+1,j]), dim=0)\n",
        "            edge2[i,j] = self.lin200(concat)\n",
        "        # Unary, edge1, edge2 = torch.Size([32, 32, 2]) torch.Size([32, 31, 2]) torch.Size([31, 32, 2])\n",
        "        return unary, edge1, edge2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rljo5bqJP3NK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_cnn(model, train_batches, test_batches, num_epochs):\n",
        "    \"\"\"\n",
        "    This function runs a training loop for the FCN semantic segmentation model\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss(weight=torch.Tensor([1, 4]))\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for i, batch in enumerate(train_batches):\n",
        "            optimizer.zero_grad()\n",
        "            images, labels = batch\n",
        "            # Images: torch.Size([1, 3, 32, 32]), Labels = torch.Size([1, 32, 32])\n",
        "            output = model(images)\n",
        "            # output: 32*32 x 2 = torch.Size([1024, 2])\n",
        "            labels = labels.contiguous().view(-1, 1).squeeze()\n",
        "            # labels: torch.Size([1024])\n",
        "            loss = criterion(output, labels)  # inputs: torch.Size([1024, 2]) and torch.Size([1024])\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(\"Epoch: {}, itr: {}\".format(epoch, i))\n",
        "        print(\"Training loss after epoch {}: {}\".format(epoch, total_loss/len(train_batches)))\n",
        "        test_cnn(model, train_batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcUqpbKxPnH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_cnn(model, test_batches):\n",
        "    \"\"\"\n",
        "        This function evaluates the FCN semantic segmentation model on the test set\n",
        "    \"\"\"\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    class_gold = [0.0] * NUM_CLASSES\n",
        "    class_pred = [0.0] * NUM_CLASSES\n",
        "    class_correct = [0.0] * NUM_CLASSES\n",
        "    for i, batch in enumerate(test_batches):\n",
        "        images, labels = batch\n",
        "        output = model(images)\n",
        "        _, output = torch.max(output, axis=1)\n",
        "        visualize_grayscale_image(output.view(32, 32).detach().numpy(), i)\n",
        "        output = output.squeeze().detach().numpy()\n",
        "        labels = labels.contiguous().view(-1, 1).squeeze().numpy()\n",
        "        cur_class_pred = np.unique(output, return_counts=True)\n",
        "        for key, val in zip(cur_class_pred[0], cur_class_pred[1]):\n",
        "            class_pred[key] += val\n",
        "        cur_class_gold = np.unique(labels, return_counts=True)\n",
        "        for key, val in zip(cur_class_gold[0], cur_class_gold[1]):\n",
        "            class_gold[key] += val\n",
        "        cur_correct = (output == labels).tolist()\n",
        "        for j, val in enumerate(cur_correct):\n",
        "            if val:\n",
        "                class_correct[labels[j]] += 1\n",
        "        correct += np.sum(cur_correct)\n",
        "        total += len(labels)\n",
        "    class_iou = [x/(y+z-x) for x, y, z in zip(class_correct, class_gold, class_pred)]\n",
        "    mean_iou = sum(class_iou) / len(class_correct)\n",
        "    print(\"Mean IOU: {}\".format(mean_iou))\n",
        "    print(\"Pixel Accuracy: {}\\n\".format(correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud2IwN-kP97o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_linear_svm(cnn_model, svm_model, train_batches, test_batches, num_epochs):\n",
        "    # TODO: Write a training loop for the linear SVM\n",
        "    # Keep in mind that the CNN model is needed to compute features, but it should not be finetuned\n",
        "    criterion = nn.MultiMarginLoss(weight=torch.Tensor([1, 4]))  # Class weights to handle class imbalance\n",
        "    optimizer = optim.Adam(svm_model.parameters(), lr=0.0001)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for i, batch in enumerate(train_batches):\n",
        "            optimizer.zero_grad()\n",
        "            images, labels = batch\n",
        "            # Images: torch.Size([1, 3, 32, 32]), Labels = torch.Size([1, 32, 32])\n",
        "            fcn_output = cnn_model.get_repr(images)  # returns 1 x 32 x 32 x 100\n",
        "            output = svm_model(fcn_output)       # torch.Size([1024, 2])\n",
        "            labels = labels.contiguous().view(-1, 1).squeeze()   # labels: torch.Size([1024])\n",
        "            loss = criterion(output, labels)    # inputs = torch.Size([1024, 2]) and torch.Size([1024])\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # print(\"Epoch: {}, itr: {}\".format(epoch, i))\n",
        "        print(\"Training loss after epoch {}: {}\".format(epoch, total_loss/len(train_batches)))\n",
        "        test_linear_svm(cnn_model, svm_model, train_batches)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDHSYvVUQAt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_linear_svm(cnn_model, svm_model, test_batches):\n",
        "    # TODO: Write a testing function for the linear SVM\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    class_gold = [0.0] * NUM_CLASSES\n",
        "    class_pred = [0.0] * NUM_CLASSES\n",
        "    class_correct = [0.0] * NUM_CLASSES\n",
        "    for i, batch in enumerate(test_batches):\n",
        "        images, labels = batch\n",
        "        fcn_output = cnn_model.get_repr(images)  # returns 1 x 32 x 32 x 100\n",
        "        output = svm_model(fcn_output)       # torch.Size([1024, 2])\n",
        "        _, output = torch.max(output, axis=1)\n",
        "        visualize_grayscale_image(output.view(32, 32).detach().numpy(), i)\n",
        "        output = output.squeeze().detach().numpy()\n",
        "        labels = labels.contiguous().view(-1, 1).squeeze().numpy()\n",
        "\n",
        "        cur_class_pred = np.unique(output, return_counts=True)\n",
        "        for key, val in zip(cur_class_pred[0], cur_class_pred[1]):\n",
        "            class_pred[key] += val\n",
        "        cur_class_gold = np.unique(labels, return_counts=True)\n",
        "        for key, val in zip(cur_class_gold[0], cur_class_gold[1]):\n",
        "            class_gold[key] += val\n",
        "        cur_correct = (output == labels).tolist()\n",
        "        for j, val in enumerate(cur_correct):\n",
        "            if val:\n",
        "                class_correct[labels[j]] += 1\n",
        "        correct += np.sum(cur_correct)\n",
        "        total += len(labels)\n",
        "    class_iou = [x/(y+z-x) for x, y, z in zip(class_correct, class_gold, class_pred)]\n",
        "    mean_iou = sum(class_iou) / len(class_correct)\n",
        "    print(\"Mean IOU: {}\".format(mean_iou))\n",
        "    print(\"Pixel Accuracy: {}\\n\".format(correct / total))\n",
        "\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-FR5TxbLTTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_aug_map_inference(label_ind, unary, edge1, edge2):\n",
        "    solver = pywraplp.Solver('LinearExample', pywraplp.Solver.GLOP_LINEAR_PROGRAMMING)\n",
        "    # Define assignment variables\n",
        "    x_assign = {}\n",
        "    for i in range(32):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          x_assign[i, j, c] = solver.NumVar(0.0, 1.0, 'x[%d,%d, %d]' % (i, j, c))\n",
        "    # Define assignment variables\n",
        "    y1_assign = {}\n",
        "    for i in range(32):\n",
        "      for j in range(31):\n",
        "        for c in range(2):\n",
        "          y1_assign[i, j, c] = solver.NumVar(0.0, 1.0, 'y1[%d,%d, %d]' % (i, j, c))\n",
        "    # Define assignment variables\n",
        "    y2_assign = {}\n",
        "    for i in range(31):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          y2_assign[i, j, c] = solver.NumVar(0.0, 1.0, 'y2[%d,%d, %d]' % (i, j, c))\n",
        "\n",
        "    # Add constraints\n",
        "    for i in range(32):\n",
        "      for j in range(32):\n",
        "        solver.Add(solver.Sum([x_assign[i,j,c] for c in range(2)])==1)\n",
        "    # Add constraints\n",
        "    for i in range(32):\n",
        "      for j in range(31):\n",
        "        for c in range(2):\n",
        "          solver.Add(y1_assign[i,j,c]<=x_assign[i,j,c])\n",
        "          solver.Add(y1_assign[i,j,c]<=x_assign[i,j+1,c])\n",
        "    # Add constraints\n",
        "    for i in range(31):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          solver.Add(y2_assign[i,j,c]<=x_assign[i,j,c])\n",
        "          solver.Add(y2_assign[i,j,c]<=x_assign[i+1,j,c])\n",
        "\n",
        "    # Define objective\n",
        "    obj = 0.0\n",
        "    for i in range(32):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          obj += unary[i,j,c]*x_assign[i,j,c]\n",
        "          obj += (1/(32*32*2))*(label_ind[i,j,c]*(1-x_assign[i,j,c]) \n",
        "                                        + (1-label_ind[i,j,c])*x_assign[i,j,c])\n",
        "    for i in range(32):\n",
        "      for j in range(31):\n",
        "        for c in range(2):\n",
        "          obj += edge1[i,j,c]*y1_assign[i,j,c]\n",
        "    for i in range(31):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          obj += edge2[i,j,c]*y2_assign[i,j,c]\n",
        "    # obj += np.sum(unary*x_assign)\n",
        "    # obj += torch.sum(edge1*y1_assign)\n",
        "    # obj += torch.sum(edge2*y2_assign)\n",
        "    # Loss augmented objective\n",
        "    # obj += (1/(32*32*2))*torch.sum(label_ind*(1-x_assign) + (1-label_ind)*x_assign)\n",
        "    solver.Maximize(obj)\n",
        "    \n",
        "    # Solve ILP again\n",
        "    result = solver.Solve()\n",
        "    assert result == pywraplp.Solver.OPTIMAL\n",
        "    assert solver.VerifySolution(1e-7, True)\n",
        "\n",
        "    # Gather the optimal assignment\n",
        "    final_x_assign = torch.zeros((32, 32, 2))\n",
        "    final_y1_assign = torch.zeros((32, 31, 2))\n",
        "    final_y2_assign = torch.zeros((31, 32, 2))\n",
        "    for i in range(32):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          final_x_assign[i, j, c] = x_assign[i,j,c].solution_value()\n",
        "    for i in range(32):\n",
        "      for j in range(31):\n",
        "        for c in range(2):\n",
        "          final_y1_assign[i, j, c] = y1_assign[i,j,c].solution_value()\n",
        "    for i in range(31):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          final_y2_assign[i, j, c] = y2_assign[i,j,c].solution_value()\n",
        "\n",
        "    return final_x_assign, final_y1_assign, final_y2_assign"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXlETq3tQDBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Write a function to compute the structured hinge loss\n",
        "# using the max-scoring output from the ILP and the gold output\n",
        "def compute_struct_svm_loss(unary, edge1, edge2, label_ind, edge1_ind, edge2_ind,\n",
        "                                           final_x_assign, final_y1_assign, final_y2_assign):\n",
        "  # Score term (Predicted)\n",
        "  score_pred = 0.0\n",
        "  score_pred += torch.sum(unary*final_x_assign)\n",
        "  score_pred += torch.sum(edge1*final_y1_assign)\n",
        "  score_pred += torch.sum(edge2*final_y2_assign)\n",
        "  # Hamming loss term\n",
        "  hloss = 0.0\n",
        "  for i in range(32):\n",
        "      for j in range(32):\n",
        "        for c in range(2):\n",
        "          if(final_x_assign[i,j,c] != label_ind[i,j,c]):\n",
        "            hloss += 1\n",
        "  hloss = hloss/(32*32*2)\n",
        "  # Score term (Gold)\n",
        "  score_gold = 0.0\n",
        "  label_ind = torch.tensor(label_ind)\n",
        "  score_gold += torch.sum(unary*label_ind)\n",
        "  score_gold += torch.sum(edge1*edge1_ind)\n",
        "  score_gold += torch.sum(edge2*edge2_ind)\n",
        "  # Total loss\n",
        "  total_loss = score_pred + hloss - score_gold\n",
        "\n",
        "  return torch.max(torch.Tensor([0]), total_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIHVXtM8QGMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_struct_svm(cnn_model, svm_model, train_batches, test_batches, num_epochs):\n",
        "    # TODO: Write a training loop for the structured SVM\n",
        "    # Keep in mind that the CNN model is needed to compute features, but it should not be finetuned\n",
        "\n",
        "    optimizer = optim.Adam(svm_model.parameters(), lr=0.0001)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for itr, batch in enumerate(train_batches):\n",
        "            optimizer.zero_grad()\n",
        "            images, labels = batch\n",
        "            # Images: torch.Size([1, 3, 32, 32]), Labels = torch.Size([1, 32, 32])\n",
        "            fcn_output = cnn_model.get_repr(images)  # returns 1 x 32 x 32 x 100\n",
        "            unary, edge1, edge2 = svm_model(fcn_output)\n",
        "            # Unary, edge1, edge2 = torch.Size([32, 32, 2]) torch.Size([32, 31, 2]) torch.Size([31, 32, 2])          \n",
        "            # unary = unary.squeeze()\n",
        "            # edge1 = edge1.squeeze()\n",
        "            # edge2 = edge2.squeeze()\n",
        "            labels = labels.squeeze()   # Labels = torch.Size([32, 32])\n",
        "\n",
        "            # Perform loss-augmented MAP inference via ILP\n",
        "            unary_np = unary.detach().numpy()\n",
        "            edge1_np = edge1.detach().numpy()\n",
        "            edge2_np = edge2.detach().numpy()\n",
        "            # Construct the Gold X indicators\n",
        "            label_ind = np.zeros((32,32,2))\n",
        "            for i in range(32):\n",
        "              for j in range(32):\n",
        "                label_ind[i,j,labels[i,j]] = 1.0\n",
        "            \n",
        "            # Loss-augmented MAP inference\n",
        "            final_x_assign, final_y1_assign, final_y2_assign = loss_aug_map_inference(label_ind, unary_np, edge1_np, edge2_np)\n",
        "\n",
        "            # Construct the Gold Y indicators\n",
        "            edge1_ind = torch.zeros((32,31,2))\n",
        "            edge2_ind = torch.zeros((31,32,2))\n",
        "            for i in range(32):\n",
        "              for j in range(31):\n",
        "                if labels[i,j]==1 and labels[i,j+1]==1:\n",
        "                  edge1_ind[i,j,1] = 1\n",
        "                if labels[i,j]==0 and labels[i,j+1]==0:\n",
        "                  edge1_ind[i,j,0] = 1\n",
        "            for i in range(31):\n",
        "              for j in range(32):\n",
        "                if labels[i,j]==1 and labels[i+1,j]==1:\n",
        "                  edge2_ind[i,j,1] = 1\n",
        "                if labels[i,j]==0 and labels[i+1,j]==0:\n",
        "                  edge2_ind[i,j,0] = 1\n",
        "            # Structured Hinge Loss\n",
        "            loss = compute_struct_svm_loss(unary, edge1, edge2, label_ind, edge1_ind, edge2_ind,\n",
        "                                           final_x_assign, final_y1_assign, final_y2_assign)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(\"Epoch: {}, itr: {}\".format(epoch, itr))\n",
        "        print(\"Training loss after epoch {}: {}\".format(epoch, total_loss/len(train_batches)))\n",
        "        # test_linear_svm(cnn_model, svm_model, train_batches)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBjhoX75rI0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_struct_svm(cnn_model, svm_model, test_batches):\n",
        "    # TODO: Write a testing function for the structured SVM\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsdqzj1UQN6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_grayscale_image(image, file=None):\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    # Uncomment this to visualize image\n",
        "    # plt.show()\n",
        "    # Uncomment this to save image\n",
        "    # plt.savefig(str(file)+\".png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EurepqnDQQbv",
        "colab_type": "code",
        "outputId": "c6646ade-ec61-4b78-8f6f-d20b5ff86071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    seed = 42\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Uncomment following lines after providing appropriate paths\n",
        "    path_to_image_folder = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledImages\"\n",
        "    path_to_label_folder = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/DownsampledLabels\"\n",
        "    file_with_train_ids = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/train.txt\"\n",
        "    file_with_test_ids = \"/content/drive/My Drive/00_CMU/03_ML_structured_data/HW3/test.txt\"\n",
        "    \n",
        "    train_dataset = VOCDataset(path_to_image_folder, path_to_label_folder, file_with_train_ids)\n",
        "    test_dataset = VOCDataset(path_to_image_folder, path_to_label_folder, file_with_test_ids)\n",
        "\n",
        "    train_batches = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "    test_batches = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    cnn = SimpleSegmentationModel(NUM_FEATURES, NUM_CLASSES)\n",
        "    train_cnn(cnn, train_batches, test_batches, 2)\n",
        "    test_cnn(cnn, test_batches)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-f454ec2abbfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleSegmentationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_FEATURES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mtest_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-95663d015842>\u001b[0m in \u001b[0;36mtrain_cnn\u001b[0;34m(model, train_batches, test_batches, num_epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-ff75b060ddb8>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# 0 stands for background, 1 for foreground\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnsRxJ2DoN69",
        "colab_type": "code",
        "outputId": "af067286-ab6e-4db9-b417-dd3371344287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "# TODO: Instantiate a linear SVM and call train/ test functions\n",
        "linear_svm = LinearSVM(n_feat = NUM_FEATURES, n_classes = NUM_CLASSES)\n",
        "train_linear_svm(cnn, linear_svm, train_batches, test_batches, num_epochs = 3)\n",
        "test_linear_svm(cnn, linear_svm, test_batches)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss after epoch 0: 0.6509752579640933\n",
            "Mean IOU: 0.3989965572194325\n",
            "Pixel Accuracy: 0.6033462982832618\n",
            "\n",
            "Training loss after epoch 1: 0.6299050095894818\n",
            "Mean IOU: 0.39956837888217933\n",
            "Pixel Accuracy: 0.6042972941255365\n",
            "\n",
            "Training loss after epoch 2: 0.6291903404207189\n",
            "Mean IOU: 0.4011234037881908\n",
            "Pixel Accuracy: 0.6067554653969958\n",
            "\n",
            "Mean IOU: 0.38932917204176226\n",
            "Pixel Accuracy: 0.596430102915952\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq_wDnXtwFKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "d511c6e2-8b30-457e-8429-2f8a1a21e578"
      },
      "source": [
        "# TODO: Instantiate a structured SVM and call train/ test functions\n",
        "struct_svm = StructSVM(n_feat = NUM_FEATURES, n_classes = NUM_CLASSES, w = 32, h = 32)\n",
        "train_struct_svm(cnn, struct_svm, train_batches, test_batches, num_epochs = 3)\n",
        "# test_struct_svm(cnn, struct_svm, test_batches)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8fe5d5572d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstruct_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStructSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUM_FEATURES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_struct_svm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# test_struct_svm(cnn, struct_svm, test_batches)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'StructSVM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M54LL86lrdaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}